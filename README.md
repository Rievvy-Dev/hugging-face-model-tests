# ğŸ¯ Pipeline Completo: AvaliaÃ§Ã£o, SeleÃ§Ã£o e Fine-Tuning de Modelos de TraduÃ§Ã£o ENâ†’PT

## ğŸ“š VisÃ£o Geral da Metodologia

Este projeto implementa um **pipeline de 5 estÃ¡gios** para identificar os melhores modelos de traduÃ§Ã£o automÃ¡tica inglÃªsâ†’portuguÃªs e adapta-los a um domÃ­nio especÃ­fico (abstracts cientÃ­ficos do SciELO).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                             â”‚
â”‚  STAGE 1: AVALIAÃ‡ÃƒO INICIAL                                               â”‚
â”‚  â”œâ”€ Testar 6 modelos em 4 datasets diferentes                             â”‚
â”‚  â”œâ”€ Calcular BLEU, chr-F, COMET, BERTScore F1                             â”‚
â”‚  â””â”€ Resultado: translation_metrics_all.csv                                â”‚
â”‚         â†“                                                                  â”‚
â”‚  STAGE 2: SELEÃ‡ÃƒO DOS MELHORES MODELOS                                    â”‚
â”‚  â”œâ”€ Usar ranking composto para escolher Top 2                             â”‚
â”‚  â”œâ”€ Salvar configuraÃ§Ãµes em JSON                                          â”‚
â”‚  â””â”€ Resultado: top2_models.json                                           â”‚
â”‚         â†“                                                                  â”‚
â”‚  STAGE 3: PREPARAÃ‡ÃƒO DE DADOS                                             â”‚
â”‚  â”œâ”€ Separar SciELO em 3 splits nÃ£o-sobrepostos:                           â”‚
â”‚  â”‚  â”œâ”€ 200k exemplos para TREINO (fine-tuning)                            â”‚
â”‚  â”‚  â”œâ”€ 20k exemplos para VALIDAÃ‡ÃƒO (monitoramento durante treino)         â”‚
â”‚  â”‚  â””â”€ 20k exemplos para TESTE (avaliaÃ§Ã£o final)                          â”‚
â”‚  â””â”€ Resultado: 3 arquivos CSV                                             â”‚
â”‚         â†“                                                                  â”‚
â”‚  STAGE 4: FINE-TUNING                                                     â”‚
â”‚  â”œâ”€ Fine-tune dos 2 modelos selecionados                                  â”‚
â”‚  â”œâ”€ Salvar checkpoints para resumir se interrompido                        â”‚
â”‚  â”œâ”€ Treinar com 200k dados + validaÃ§Ã£o com 20k                             â”‚
â”‚  â””â”€ Resultado: modelos fine-tuned salvos                                  â”‚
â”‚         â†“                                                                  â”‚
â”‚  STAGE 5: AVALIAÃ‡ÃƒO FINAL E COMPARAÃ‡ÃƒO                                    â”‚
â”‚  â”œâ”€ Testar modelos fine-tuned nos MESMOS 20k dados de teste               â”‚
â”‚  â”œâ”€ Comparar com resultados do STAGE 1 (base vs fine-tuned)               â”‚
â”‚  â”œâ”€ Detectar overfitting/underfitting                                     â”‚
â”‚  â””â”€ Resultado: relatÃ³rio comparativo final                                â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quickstart (Rodar Tudo)

Se vocÃª deseja executar o pipeline completo:

```bash
# 1. Instalar dependÃªncias
pip install -r requirements.txt
pip install -r requirements-ml.txt

# 2. Preparar dataset Scielo (gera abstracts_scielo.csv)
python prepare_scielo_dataset.py

# 3. Executar pipeline completo (teste â†’ seleÃ§Ã£o â†’ preparaÃ§Ã£o â†’ fine-tuning â†’ teste final)
python finetune_and_evaluate.py --skip_prepare
```

Se as etapas anteriores nÃ£o falharem, os resultados finais estarÃ£o em:
- `scielo_before_finetuning.csv` - MÃ©tricas dos modelos base
- `scielo_after_finetuning.csv` - MÃ©tricas dos modelos fine-tuned
- `SCIENCE_EVALUATION_REPORT.txt` - RelatÃ³rio comparativo

---

## ğŸ“‹ STAGE 1: AvaliaÃ§Ã£o Inicial dos Modelos

### O que faz:
Avalia **6 modelos prÃ©-treinados** em **4 datasets pÃºblicos** para estabelecer baseline.

### Modelos testados:
1. `Helsinki-NLP/opus-mt-tc-big-en-pt` (MarianMT)
2. `Narrativa/mbart-large-50-finetuned-opus-en-pt-translation` (mBART)
3. `unicamp-dl/translation-en-pt-t5` (T5)
4. `VanessaSchenkel/unicamp-finetuned-en-to-pt-dataset-ted` (T5 fine-tuned TED)
5. `danhsf/m2m100_418M-finetuned-kde4-en-to-pt_BR` (M2M100)
6. `quickmt/quickmt-en-pt` (CTranslate2)

### Datasets pÃºblicos:
- **WMT24++** (en-pt_BR): 998 exemplos
- **ParaCrawl** (enâ†’pt): 5000 exemplos  
- **Flores** (Facebook): 1012 exemplos
- **OPUS100** (en-pt): 5000 exemplos

### MÃ©tricas calculadas:
- **BLEU**: PrecisÃ£o de n-gramas (0-100)
- **chr-F**: F-score baseado em caracteres (0-100)
- **COMET**: Score neural aprendido (0-1)
- **BERTScore F1**: Similaridade semÃ¢ntica (0-1)

### Executar STAGE 1:

```bash
# Avaliar os 5 modelos primÃ¡rios
python models-test.py --resume

# ou para refazer do zero
python models-test.py --full

# Avaliar o 6Âº modelo (QuickMT)
python evaluate_quickmt.py --resume

# ou para refazer
python evaluate_quickmt.py --full
```

### SaÃ­da gerada:
- `evaluation_results/translation_metrics_all.csv` - Consolidado com todos os resultados
- `evaluation_results/<modelo>.csv` - Resultados por modelo individuais

---

## ğŸ† STAGE 2: SeleÃ§Ã£o dos Melhores Modelos

### O que faz:
Analisa os resultados do STAGE 1 e identifica os **2 melhores modelos** usando score composto.

### Scoring:
```
score = 0.30Ã—BLEU + 0.25Ã—chr-F + 0.25Ã—COMET + 0.20Ã—BERTScore F1
```

Todos os scores sÃ£o **normalizados min-max** para [0,1] antes de combinar.

### Executar STAGE 2:

```bash
# Analisar e escolher top 2
python choose_best_model.py

# Ou com arquivo customizado
python choose_best_model.py evaluation_results/translation_metrics_all.csv
```

### SaÃ­da:
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  RANKING GERAL - Score composto (BLEU + chr-F + COMET + BERTScore)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  1. danhsf/m2m100_418M-finetuned-kde4-en-to-pt_BR *
     Score: 0.8765  |  BLEU: 29.42  |  chr-F: 50.21  |  COMET: 0.7645 |  BERTScore: 0.8301

  2. Helsinki-NLP/opus-mt-tc-big-en-pt *
     Score: 0.8321  |  BLEU: 33.78  |  chr-F: 59.89  |  COMET: 0.7825 |  BERTScore: 0.8622
```

### PrÃ³ximo passo:
Os 2 modelos selecionados serÃ£o fine-tuned no STAGE 4.

---

## ğŸ—‚ï¸ STAGE 3: PreparaÃ§Ã£o de Dados SciELO

### O que faz:
Separa o dataset **abstracts_scielo.csv** em 3 splits nÃ£o-sobrepostos:

```
abstracts_scielo.csv (2.7M exemplos)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DivisÃ£o ESTRATIFICADA                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ TREINO:        200,000 exemplos       â”‚  
â”‚ â€¢ VALIDAÃ‡ÃƒO:      20,000 exemplos       â”‚  (monitora convergÃªncia)
â”‚ â€¢ TESTE:          20,000 exemplos       â”‚  (avaliaÃ§Ã£o final)
â”‚                                          â”‚
â”‚ Total: 240,000 exemplos (~8.7%)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Salvo em: finetuning/abstracts-datasets/
    â”œâ”€ scielo_abstracts_train.csv
    â”œâ”€ scielo_abstracts_val.csv
    â””â”€ scielo_abstracts_test.csv
```

### CaracterÃ­sticas importantes:
- **Sem sobreposiÃ§Ã£o**: Cada exemplo aparece em apenas 1 split
- **Seed fixo (42)**: Reprodutibilidade
- **Estratificado**: MantÃ©m distribuiÃ§Ã£o de comprimento equilibrada
- **DeterminÃ­stico**: Sempre gera os mesmos splits

### Executar STAGE 3:

```bash
# Via select_and_test_models.py (prepara automaticamente)
python finetuning/select_and_test_models.py

# Ou manualmente via datasets.prepare_evaluation_csv
python -c "
from finetuning import config, datasets
datasets.prepare_evaluation_csv(
    abstracts_file='abstracts_scielo.csv',
    train_csv=config.SCIELO_TRAIN_CSV,
    val_csv=config.SCIELO_VAL_CSV,
    test_csv=config.SCIELO_TEST_CSV,
    train_samples=200_000,
    val_samples=20_000,
    test_samples=20_000
)
"
```

### SaÃ­da:
- `finetuning/abstracts-datasets/scielo_abstracts_train.csv` (200k linhas)
- `finetuning/abstracts-datasets/scielo_abstracts_val.csv` (20k linhas)
- `finetuning/abstracts-datasets/scielo_abstracts_test.csv` (20k linhas)

### Testar modelos base individualmente:

```bash
# Testar todos os modelos (helsinki + m2m100)
python finetuning/select_and_test_models.py --skip_prepare

# Testar apenas Helsinki
python finetuning/select_and_test_models.py --skip_prepare --model helsinki

# Testar apenas M2M100
python finetuning/select_and_test_models.py --skip_prepare --model m2m100
```

**SaÃ­da**: `scielo_before_finetuning.csv` com mÃ©tricas BLEU, chrF, COMET, BERTScore

---

## ğŸ“ STAGE 4: Fine-tuning dos Melhores Modelos

### O que faz:
Treina os 2 modelos selecionados no STAGE 2 usando dados de STAGE 3.

### Arquitetura:
- **Seq2SeqTrainer** do HuggingFace
- **Mixed precision training** (FP16 quando possÃ­vel)
- **Gradient accumulation** se necessÃ¡rio
- **Checkpoints** salvos a cada Ã©poca

### ConfiguraÃ§Ãµes padrÃ£o:
```python
EPOCHS = 5
BATCH_SIZE = 2
EVAL_BATCH_SIZE = 2
LEARNING_RATE = 2e-5
WARMUP_STEPS = 500
MAX_SEQ_LENGTH = 256
```

### Executar STAGE 4:

```bash
# Fine-tuning de ambos os modelos
python finetuning/finetune_selected_models.py --skip_prepare

# Fine-tuning do modelo especÃ­fico
python finetuning/finetune_selected_models.py --model helsinki --skip_prepare
python finetuning/finetune_selected_models.py --model m2m100 --skip_prepare

# Com parÃ¢metros customizados
python finetuning/finetune_selected_models.py \
  --model helsinki \
  --epochs 10 \
  --batch_size 4 \
  --lr 5e-5 \
  --skip_prepare

# Retomar fine-tuning interrompido
python finetuning/finetune_selected_models.py \
  --model helsinki \
  --resume_from ./models/finetuned-scielo/helsinki/checkpoint-3000 \
  --skip_prepare
```

### SaÃ­da:
```
models/finetuned-scielo/
â”œâ”€â”€ helsinki/
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”œâ”€â”€ checkpoint-1000/
â”‚   â”œâ”€â”€ checkpoint-2000/
â”‚   â””â”€â”€ ...
â””â”€â”€ m2m100/
    â”œâ”€â”€ config.json
    â”œâ”€â”€ pytorch_model.bin
    â””â”€â”€ ...
```

### Checkpoints:
- Salvos a cada `eval_steps` (~1/5 da Ã©poca por padrÃ£o)
- Permitem **resumir treino** se interrompido
- Incluem optimizer state para convergÃªncia suave

---

## ğŸ“Š STAGE 5: AvaliaÃ§Ã£o Final e ComparaÃ§Ã£o

### O que faz:
Avalia os modelos fine-tuned **nos mesmos 20k dados de teste** do STAGE 3 e compara com STAGE 1.

### Crucial: Usar os MESMOS dados de teste
```
STAGE 1 (modelos base):                STAGE 5 (modelos fine-tuned):
â”œâ”€ Testar em: 20k SciELO teste   vs   â”œâ”€ Testar em: MESMOS 20k SciELO
â”œâ”€ Resultado: BLEU=X.xx              â”œâ”€ Resultado: BLEU=Y.yy
â””â”€ Arquivo: scielo_before_*           â””â”€ Arquivo: scielo_after_*

Delta BLEU = Y.yy - X.xx
Se Delta > 20%: âš ï¸ PossÃ­vel overfitting
Se Delta < 0%: âŒ Underfitting / problemas
```

### Executar STAGE 5:

```bash
# Testar nos dados SciELO (todos os modelos)
python finetuning/select_and_test_models.py --test_finetuned --skip_prepare

# Testar modelo especÃ­fico fine-tuned
python finetuning/select_and_test_models.py --test_finetuned --model helsinki --skip_prepare
python finetuning/select_and_test_models.py --test_finetuned --model m2m100 --skip_prepare

# Comparar base vs fine-tuned (todos os modelos)
python finetuning/select_and_test_models.py --test_both --skip_prepare

# Comparar base vs fine-tuned (modelo especÃ­fico)
python finetuning/select_and_test_models.py --test_both --model helsinki --skip_prepare

# Gerar CSV de comparaÃ§Ã£o
python compare_results.py
```

### SaÃ­da:
- `scielo_before_finetuning.csv` - Modelos base
- `scielo_after_finetuning.csv` - Modelos fine-tuned
- `SCIENCE_EVALUATION_REPORT.txt` - AnÃ¡lise detalhada

### Exemplo de comparaÃ§Ã£o:
```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
COMPARAÃ‡ÃƒO: Base vs Fine-tuned (SciELO)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

danhsf/m2m100_418M-finetuned-kde4-en-to-pt_BR
  ANTES:   BLEU=22.99  chr-F=50.08
  DEPOIS:  BLEU=28.45  chr-F=54.32
  DELTA:   +23.8% (possÃ­vel overfitting)  âš ï¸

Helsinki-NLP/opus-mt-tc-big-en-pt
  ANTES:   BLEU=33.71  chr-F=58.86
  DEPOIS:  BLEU=35.12  chr-F=60.21
  DELTA:   +4.2% (melhoria moderada)  âœ…
```

---

## ğŸ”§ Estrutura do Projeto

```
.
â”œâ”€â”€ ğŸ“„ README.md (este arquivo)
â”œâ”€â”€ ğŸ“„ requirements.txt
â”œâ”€â”€ ğŸ“„ requirements-ml.txt
â”‚
â”œâ”€â”€ ğŸ prepare_scielo_dataset.py           [STAGE 0] Gerar abstracts_scielo.csv
â”œâ”€â”€ ğŸ models-test.py                      [STAGE 1] Avaliar 5 modelos
â”œâ”€â”€ ğŸ evaluate_quickmt.py                 [STAGE 1] Avaliar 6Âº modelo
â”œâ”€â”€ ğŸ choose_best_model.py                [STAGE 2] Selecionar Top 2
â”œâ”€â”€ ğŸ finetune_and_evaluate.py            [STAGES 1-5] Pipeline integrado
â”œâ”€â”€ ğŸ compare_results.py                  [STAGE 5] Gerar relatÃ³rio
â”‚
â”œâ”€â”€ ğŸ“Š abstracts_scielo.csv                Dataset Scielo completo (2.7M)
â”œâ”€â”€ ğŸ“‚ evaluation_results/
â”‚   â”œâ”€â”€ translation_metrics_all.csv        [STAGE 1] Resultado consolidado
â”‚   â”œâ”€â”€ <modelo>.csv                       [STAGE 1] Resultados por modelo
â”‚   â”œâ”€â”€ scielo_before_finetuning.csv       [STAGE 5] Modelos base em SciELO
â”‚   â””â”€â”€ scielo_after_finetuning.csv        [STAGE 5] Modelos fine-tuned em SciELO
â”‚
â”œâ”€â”€ ğŸ“¦ finetuning/                         Pacote principal
â”‚   â”œâ”€â”€ config.py                          ConfiguraÃ§Ãµes centralizadas
â”‚   â”œâ”€â”€ models.py                          Carregamento e salvamento
â”‚   â”œâ”€â”€ datasets.py                        PreparaÃ§Ã£o de dados
â”‚   â”œâ”€â”€ metrics.py                         BLEU, chr-F, COMET, BERTScore
â”‚   â”œâ”€â”€ evaluate.py                        AvaliaÃ§Ã£o com progresso (tqdm)
â”‚   â”œâ”€â”€ trainer.py                         Seq2SeqTrainer + loop fine-tuning
â”‚   â”œâ”€â”€ compare.py                         ComparaÃ§Ã£o base vs fine-tuned
â”‚   â”œâ”€â”€ io_utils.py                        UtilitÃ¡rios I/O
â”‚   â”‚
â”‚   â”œâ”€â”€ select_and_test_models.py          [STAGE 3+5] Teste SciELO
â”‚   â”œâ”€â”€ finetune_selected_models.py        [STAGE 4] Fine-tuning SciELO
â”‚   â”‚
â”‚   â””â”€â”€ abstracts-datasets/                [STAGE 3] Dados SciELO splits
â”‚       â”œâ”€â”€ scielo_abstracts_train.csv     200k exemplos
â”‚       â”œâ”€â”€ scielo_abstracts_val.csv       20k exemplos
â”‚       â””â”€â”€ scielo_abstracts_test.csv      20k exemplos
â”‚
â”œâ”€â”€ ğŸ“‚ checkpoints/                        Checkpoints de treino/validaÃ§Ã£o
â”‚   â”œâ”€â”€ training/
â”‚   â””â”€â”€ evaluation/
â”‚
â””â”€â”€ ğŸ“‚ models/finetuned-scielo/           Modelos fine-tuned
    â”œâ”€â”€ helsinki/
    â”‚   â”œâ”€â”€ config.json
    â”‚   â”œâ”€â”€ pytorch_model.bin
    â”‚   â””â”€â”€ checkpoint-*/
    â””â”€â”€ m2m100/
        â”œâ”€â”€ config.json
        â”œâ”€â”€ pytorch_model.bin
        â””â”€â”€ checkpoint-*/
```

---

## ğŸ’¡ Detalhes TÃ©cnicos Importantes

### 1. **Reprodutibilidade**
- Seed = 42 em todos os splits de dados
- Modelos carregados com `torch.manual_seed(42)`
- Resultados sÃ£o determinÃ­sticos

### 2. **Sem SobreposiÃ§Ã£o de Dados**
```python
Total: 2.7M exemplos
Usar: 240k exemplos
â”œâ”€ Treino: 200k (74%)      â† Fine-tuning
â”œâ”€ Val:     20k (8%)       â† Monitorar convergÃªncia
â””â”€ Teste:   20k (8%)       â† MESMOS dados em STAGE 1 e 5
```

**Importante**: O split de TESTE no STAGE 3 Ã© o **mesmo** usado para testar modelos base no STAGE 1, permitindo comparaÃ§Ã£o justa.

### 3. **Checkpoints e Resumir**
```bash
# Se o treino for interrompido (power failure, timeout, etc)
# Localizar o checkpoint mais recente
ls models/finetuned-scielo/helsinki/

# Retomar exatamente de onde parou
python finetuning/finetune_selected_models.py \
  --model helsinki \
  --resume_from ./models/finetuned-scielo/helena/checkpoint-5000 \
  --skip_prepare
```

### 4. **Detectar Overfitting**
Comparar BLEU de STAGE 1 (base) vs STAGE 5 (fine-tuned):
- **+5% a +15%**: Melhoria saudÃ¡vel âœ…
- **+15% a +20%**: PossÃ­vel overfitting âš ï¸
- **> +20%**: ProvÃ¡vel overfitting âŒ (rediferenciar dados)
- **< 0%**: Underfitting âŒ (aumentar Ã©pocas/dados)

---

## ğŸ› ï¸ Troubleshooting

### CUDA Out Of Memory
```bash
# Batch size jÃ¡ estÃ¡ em 2 (padrÃ£o)
# Se ainda der OOM, tente batch_size=1
python finetuning/finetune_selected_models.py --batch_size 1

# Ou usar CPU (lento!)
export CUDA_VISIBLE_DEVICES=-1
python finetuning/finetune_selected_models.py --batch_size 2
```

### Dataset nÃ£o encontrado
```bash
# Gerar abstracts_scielo.csv
python prepare_scielo_dataset.py

# Verificar
ls -lh abstracts_scielo.csv
```

### Modelo nÃ£o carrega
```bash
# Limpar cache HF
rm -rf ~/.cache/huggingface/

# Tentar novamente (vai baixar modelo)
python finetuning/select_and_test_models.py --skip_prepare

# Ou testar modelo especÃ­fico
python finetuning/select_and_test_models.py --skip_prepare --model helsinki
```

### Treino muito lento
- Reduzir `--train_samples` para teste (ex: 50k)
- Batch size jÃ¡ estÃ¡ otimizado (2)
- GPU com Tensor Cores (A100, RTX 3090) Ã© 10x mais rÃ¡pido
- Use `--model helsinki` ou `--model m2m100` para treinar 1 modelo por vez

---

## ğŸ“š ReferÃªncias

- **HuggingFace Transformers**: https://huggingface.co/docs/transformers/
- **SACREBleu**: https://github.com/mjpost/sacrebleu
- **COMET**: https://github.com/Unbabel/COMET
- **BERTScore**: https://github.com/Tiiiger/bert_score

---

## ğŸ“ Reproduzindo este Trabalho

Para rodar exatamente como descrito:

```bash
# 1. Clone e prepare
git clone <repo>
cd hugging-face-model-tests
pip install -r requirements.txt -r requirements-ml.txt

# 2. STAGE 0: Dataset
python prepare_scielo_dataset.py

# 3. STAGE 1: AvaliaÃ§Ã£o
python models-test.py --full
python evaluate_quickmt.py --full

# 4. STAGE 2: SeleÃ§Ã£o
python choose_best_model.py

# 5. STAGE 3: PreparaÃ§Ã£o (automÃ¡tico na prÃ³xima etapa)
# (vai ser feito por select_and_test_models.py)

# 6. STAGE 4: Fine-tuning
python finetuning/finetune_selected_models.py

# 7. STAGE 5: AvaliaÃ§Ã£o Final
python finetuning/select_and_test_models.py --test_both --skip_prepare

# 8. Gerar RelatÃ³rio
python compare_results.py
```

---

**VersÃ£o**: 3.0 | **Data**: Fevereiro 2026
