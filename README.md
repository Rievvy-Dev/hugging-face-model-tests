# Pipeline de AvaliaÃ§Ã£o e Fine-Tuning de Modelos de TraduÃ§Ã£o ENâ†’PT

## VisÃ£o Geral

Este projeto implementa um **pipeline de 5 estÃ¡gios** para avaliar e fine-tunar modelos de traduÃ§Ã£o automÃ¡tica neural (NMT) inglÃªsâ†’portuguÃªs, aplicados ao domÃ­nio de abstracts cientÃ­ficos do **SciELO**.

O modelo selecionado para fine-tuning foi o **`unicamp-dl/translation-en-pt-t5`**, uma adaptaÃ§Ã£o do T5 (Text-to-Text Transfer Transformer) para traduÃ§Ã£o ENâ†’PT, desenvolvido pela Universidade Estadual de Campinas (UNICAMP).

### MotivaÃ§Ã£o: Por que estudar traduÃ§Ã£o automÃ¡tica neural quando LLMs jÃ¡ traduzem bem?

Modelos de linguagem de grande porte (LLMs) como GPT-4 e Claude produzem traduÃ§Ãµes de alta qualidade em cenÃ¡rios gerais. Isso levanta uma questÃ£o legÃ­tima: **por que pesquisar fine-tuning de modelos NMT dedicados?** A resposta envolve mÃºltiplas dimensÃµes fundamentais para pesquisa acadÃªmica e aplicaÃ§Ãµes em escala:

#### 1. Custo e escalabilidade

O corpus SciELO contÃ©m **2,7 milhÃµes** de pares de abstracts. Traduzir esse volume via API de LLM teria custo proibitivo:

| Abordagem             | Custo estimado (2.7M abstracts)      | LatÃªncia          |
|-----------------------|--------------------------------------|--------------------|
| GPT-4 API             | ~$8.000â€“15.000 (tokens de I/O)       | Dias (rate limits) |
| Claude API            | ~$5.000â€“10.000                       | Dias (rate limits) |
| Google Translate API  | ~$4.000â€“6.000                        | Horas              |
| **NMT local (T5)**    | **$0 (apenas eletricidade)**         | **Horas (GPU)**    |

Um modelo NMT fine-tuned roda localmente em uma **GPU de ~$300** (RTX 4050) sem custo por token, sem limites de taxa, e sem dependÃªncia de serviÃ§os externos.

#### 2. Reprodutibilidade e rigor cientÃ­fico

Resultados acadÃªmicos devem ser **reprodutÃ­veis**. LLMs comerciais sÃ£o:
- **NÃ£o-determinÃ­sticos**: mesma entrada pode gerar saÃ­das diferentes (temperature > 0)
- **Opacos**: arquitetura, dados de treino e pesos sÃ£o proprietÃ¡rios
- **MutÃ¡veis**: modelos sÃ£o atualizados sem aviso â€” GPT-4 de janeiro â‰  GPT-4 de junho
- **NÃ£o-auditÃ¡veis**: impossÃ­vel inspecionar por que uma traduÃ§Ã£o especÃ­fica foi gerada

Um modelo NMT open-source com pesos fixos produz **saÃ­da determinÃ­stica** e permite **inspeÃ§Ã£o completa**: arquitetura, pesos, tokenizador, dados de treino â€” tudo verificÃ¡vel e citÃ¡vel.

#### 3. Soberania de dados e privacidade

Textos biomÃ©dicos podem conter informaÃ§Ãµes sensÃ­veis. Enviar dados para APIs externas levanta questÃµes de:
- **Privacidade**: dados podem ser retidos para treino pelos provedores
- **Conformidade legal**: LGPD e regulamentaÃ§Ãµes de dados biomÃ©dicos
- **Soberania**: dependÃªncia de infraestrutura estrangeira para processamento de dados nacionais

Modelos locais processam dados **inteiramente em hardware prÃ³prio**, sem transmissÃ£o para terceiros.

#### 4. EspecializaÃ§Ã£o de domÃ­nio

LLMs sÃ£o generalistas. Para domÃ­nios especializados como biomedicina, modelos NMT fine-tuned oferecem vantagens (Koehn & Knowles, 2017):
- **ConsistÃªncia terminolÃ³gica**: termos como "randomized controlled trial" devem ser sempre traduzidos como "ensaio clÃ­nico randomizado", nÃ£o variar entre chamadas
- **VocabulÃ¡rio de domÃ­nio**: tokenizador e embeddings ajustados para termos cientÃ­ficos
- **AvaliaÃ§Ã£o controlada**: mÃ©tricas calculÃ¡veis (BLEU, COMET) em test sets fixos

Zhu et al. (2023) demonstraram que LLMs como GPT-4 superam o NLLB em apenas **40,91%** das direÃ§Ãµes de traduÃ§Ã£o, com gap significativo para traduÃ§Ãµes especializadas e pares de idiomas com menos recursos.

#### 5. ContribuiÃ§Ã£o cientÃ­fica

A relevÃ¢ncia acadÃªmica deste trabalho nÃ£o estÃ¡ apenas nos resultados, mas na **metodologia**:
- Documentar um pipeline reprodutÃ­vel de avaliaÃ§Ã£o e fine-tuning de NMT
- Demonstrar que **tÃ©cnicas de regularizaÃ§Ã£o** importam mais que volume de dados
- Fornecer um caso de estudo empÃ­rico de **catastrophic forgetting** vs. fine-tuning bem-sucedido
- Contribuir para a pesquisa em traduÃ§Ã£o automÃ¡tica ENâ†’PT no domÃ­nio biomÃ©dico, que ainda Ã© sub-representada na literatura

> **Em resumo**: LLMs sÃ£o excelentes para traduÃ§Ã£o casual. Mas para traduÃ§Ã£o **em escala**, **reprodutÃ­vel**, **auditÃ¡vel**, **privada** e **especializada em domÃ­nio** â€” como Ã© necessÃ¡rio em pesquisa cientÃ­fica â€” modelos NMT dedicados e fine-tuned continuam sendo a abordagem mais adequada e economicamente viÃ¡vel.

### Resultados Obtidos

| MÃ©trica    | Antes do Fine-tuning | ApÃ³s Fine-tuning (Epoch 12) | Delta   | Melhoria |
|------------|---------------------:|----------------------------:|--------:|---------:|
| BLEU       | 40.06                | 45.51                       | +5.45   | +13.6%   |
| chrF       | 65.61                | 70.54                       | +4.93   | +7.5%    |
| COMET      | 0.8499               | 0.8756                      | +0.0257 | +3.0%    |
| BERTScore  | 0.8957               | 0.9124                      | +0.0167 | +1.9%    |

---

## Sobre o Modelo: `unicamp-dl/translation-en-pt-t5`

### Arquitetura

O modelo Ã© baseado na arquitetura **T5 (Text-to-Text Transfer Transformer)** proposta por Raffel et al. (2019). O T5 trata todas as tarefas de NLP como problemas de texto-para-texto, onde tanto a entrada quanto a saÃ­da sÃ£o sequÃªncias de texto.

| Componente                | EspecificaÃ§Ã£o               |
|---------------------------|:----------------------------|
| Arquitetura base          | T5 (encoder-decoder)        |
| Camadas do encoder        | 12                          |
| Camadas do decoder        | 12                          |
| DimensÃ£o oculta (d_model) | 768                         |
| CabeÃ§as de atenÃ§Ã£o        | 12                          |
| DimensÃ£o do feed-forward  | 3072                        |
| ParÃ¢metros totais         | ~223M (222.903.552)         |
| VocabulÃ¡rio               | 32.128 tokens (SentencePiece) |
| Tipo de atenÃ§Ã£o           | Multi-head self-attention   |
| NormalizaÃ§Ã£o              | Layer Normalization (pre-norm) |
| AtivaÃ§Ã£o                  | ReLU (Rectified Linear Unit) |

### O que significam os parÃ¢metros da arquitetura?

Cada campo do `config.json` do modelo define uma propriedade matemÃ¡tica especÃ­fica da rede neural. Abaixo, a explicaÃ§Ã£o de cada um com as fÃ³rmulas:

#### `d_model = 768` â€” DimensÃ£o oculta

Ã‰ o tamanho do vetor que representa cada token em **todas as camadas** do modelo. Cada palavra (token) da entrada Ã© convertida em um vetor de 768 dimensÃµes. Todas as operaÃ§Ãµes internas (atenÃ§Ã£o, feed-forward, projeÃ§Ã£o) operam nessa dimensionalidade.

$$\text{embedding}(x_i) \in \mathbb{R}^{768}$$

**Analogia**: Se cada token fosse uma pessoa, `d_model` seria quantas "caracterÃ­sticas" (altura, peso, idade, ...) descrevem essa pessoa. Com 768 caracterÃ­sticas, o modelo captura nuances semÃ¢nticas muito finas.

#### `num_heads = 12` â€” CabeÃ§as de atenÃ§Ã£o

O mecanismo de **Multi-Head Attention** (Vaswani et al., 2017) divide a atenÃ§Ã£o em mÃºltiplas "perspectivas" independentes. Cada cabeÃ§a aprende a capturar um tipo diferente de relaÃ§Ã£o linguÃ­stica:

$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_{12}) \cdot W^O$$

Onde cada cabeÃ§a Ã©:

$$\text{head}_i = \text{Attention}(Q \cdot W_i^Q, \; K \cdot W_i^K, \; V \cdot W_i^V)$$

**ReferÃªncia**: Vaswani, A. et al. (2017). *Attention is All You Need*. In NeurIPS 2017. https://arxiv.org/abs/1706.03762

**O que cada cabeÃ§a captura** (exemplos tÃ­picos do que se observa em modelos treinados):

```
Head 1:  RelaÃ§Ãµes sujeito-verbo     ("paciente" â† atenÃ§Ã£o â†’ "apresentou")
Head 2:  RelaÃ§Ãµes de adjacÃªncia     ("febre" â† atenÃ§Ã£o â†’ "persistente")
Head 3:  RelaÃ§Ãµes de correferÃªncia  ("ele" â† atenÃ§Ã£o â†’ "paciente")
Head 4:  RelaÃ§Ãµes posicionais       (palavras prÃ³ximas entre si)
Head 5:  PontuaÃ§Ã£o e estrutura      ("." â† atenÃ§Ã£o â†’ fim de sentenÃ§a)
...
Head 12: PadrÃµes aprendidos diversos
```

#### `d_kv = 64` â€” DimensÃ£o por cabeÃ§a de atenÃ§Ã£o

Cada cabeÃ§a de atenÃ§Ã£o opera num subespaÃ§o de dimensÃ£o $d_{kv}$. Ã‰ a dimensÃ£o dos vetores Query ($Q$), Key ($K$) e Value ($V$) individuais de cada cabeÃ§a.

$$d_{kv} = \frac{d_{model}}{num\_heads} = \frac{768}{12} = 64$$

O mecanismo de **Scaled Dot-Product Attention** (a operaÃ§Ã£o central de cada cabeÃ§a) Ã©:

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q \cdot K^T}{\sqrt{d_{kv}}}\right) \cdot V$$

Onde:
- $Q \in \mathbb{R}^{n \times 64}$ = queries (o que cada token "procura")
- $K \in \mathbb{R}^{n \times 64}$ = keys (o que cada token "oferece" para ser encontrado)
- $V \in \mathbb{R}^{n \times 64}$ = values (a informaÃ§Ã£o que cada token "carrega")
- $\sqrt{d_{kv}} = \sqrt{64} = 8$ = fator de escala (evita que o softmax sature)
- $n$ = comprimento da sequÃªncia

```
Exemplo: "O paciente apresentou febre"  (4 tokens, d_kv=64)

                  Kâ‚(O)   Kâ‚‚(pac.)  Kâ‚ƒ(apr.)  Kâ‚„(febre)
Qâ‚(O)          [ 0.80     0.05      0.10      0.05   ]    â†’ "O" atende a si mesmo
Qâ‚‚(paciente)   [ 0.10     0.30      0.50      0.10   ]    â†’ "paciente" atende "apresentou"
Qâ‚ƒ(apresentou) [ 0.05     0.45      0.20      0.30   ]    â†’ "apresentou" atende "paciente"
Qâ‚„(febre)      [ 0.02     0.08      0.40      0.50   ]    â†’ "febre" atende "apresentou"
                  â†‘ cada valor Ã© um peso de atenÃ§Ã£o (soma = 1 por linha, via softmax)
```

#### `d_ff = 3072` â€” DimensÃ£o do feed-forward

ApÃ³s cada bloco de atenÃ§Ã£o, o output passa por uma rede **Feed-Forward** (FFN) de duas camadas. A primeira expande a dimensionalidade, a segunda comprime de volta:

$$\text{FFN}(x) = \text{ReLU}(x \cdot W_1) \cdot W_2$$

> **Nota**: A formulaÃ§Ã£o original de Vaswani et al. (2017) inclui termos de bias ($b_1, b_2$), mas a implementaÃ§Ã£o T5 **nÃ£o usa bias** nas camadas lineares â€” apenas as matrizes de peso $W_1$ e $W_2$.

Onde:
- $W_1 \in \mathbb{R}^{768 \times 3072}$ â†’ expande 768 â†’ 3072 (4x)
- $W_2 \in \mathbb{R}^{3072 \times 768}$ â†’ comprime 3072 â†’ 768
- $\text{ReLU}(z) = \max(0, z)$ â†’ ativaÃ§Ã£o nÃ£o-linear

```
Input:  x âˆˆ â„^768    (vetor do token apÃ³s atenÃ§Ã£o)
         â†“
    Wâ‚ Ã— x            â†’ â„^3072  (expansÃ£o: 768 â†’ 3072, sem bias)
         â†“
    ReLU(Â·)           â†’ â„^3072  (nÃ£o-linearidade: zera negativos)
         â†“
    Wâ‚‚ Ã— Â·            â†’ â„^768   (compressÃ£o: 3072 â†’ 768, sem bias)
         â†“
Output: y âˆˆ â„^768    (mesmo tamanho que input â†’ residual connection)
```

**Por que 3072?** A razÃ£o $d_{ff} / d_{model} = 3072 / 768 = 4\times$ Ã© uma convenÃ§Ã£o estabelecida por Vaswani et al. (2017). A expansÃ£o temporÃ¡ria para 4x permite ao modelo aprender transformaÃ§Ãµes mais complexas, e a compressÃ£o de volta para $d_{model}$ mantÃ©m a uniformidade dimensional entre camadas.

#### `dropout_rate = 0.1` â€” RegularizaÃ§Ã£o por dropout

Durante o treino, **10% dos neurÃ´nios sÃ£o aleatoriamente desativados** (zerados) a cada forward pass. Isso forÃ§a o modelo a aprender representaÃ§Ãµes mais robustas â€” ele nÃ£o pode depender de nenhum neurÃ´nio individual.

$$\text{Dropout}(x_i) = \begin{cases} \frac{x_i}{1-p} & \text{com probabilidade } 1-p \\ 0 & \text{com probabilidade } p = 0.1 \end{cases}$$

O fator $\frac{1}{1-p} = \frac{1}{0.9} \approx 1.11$ Ã© o **inverted dropout** â€” escala os valores restantes para manter a mesma magnitude esperada (Srivastava et al., 2014).

**ReferÃªncia**: Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). *Dropout: A Simple Way to Prevent Neural Networks from Overfitting*. JMLR, 15(1), pp. 1929â€“1958.

#### `relative_attention_num_buckets = 32` â€” PosiÃ§Ã£o relativa

Diferente do Transformer original que usa embeddings posicionais absolutos (senoidais), o T5 usa **relative position bias** (Shaw et al., 2018; Raffel et al., 2019). Em vez de codificar a posiÃ§Ã£o absoluta de cada token, codifica a **distÃ¢ncia relativa** entre pares de tokens.

As distÃ¢ncias relativas sÃ£o agrupadas em 32 "buckets" (baldes) usando uma escala logarÃ­tmica:

```
DistÃ¢ncia relativa    Bucket
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€
         0              0     (mesmo token)
        Â±1              1     (adjacente)
        Â±2              2
        Â±3-4            3     (comeÃ§a a agrupar)
        Â±5-7            4
        Â±8-15           5
        Â±16-31          6
        Â±32-63          7
        ...             ...
       Â±64-128         ...    (max_distance=128)
```

A escala logarÃ­tmica permite que o modelo distinga tokens prÃ³ximos com alta resoluÃ§Ã£o, mas agrupe tokens distantes â€” o que faz sentido linguisticamente (a relaÃ§Ã£o entre palavras adjacentes Ã© mais variada que entre palavras separadas por 100 tokens).

**ReferÃªncia**: Shaw, P., Uszkoreit, J., & Vaswani, A. (2018). *Self-Attention with Relative Position Representations*. In Proceedings of NAACL-HLT 2018, pp. 464â€“468. https://aclanthology.org/N18-2074/

#### `vocab_size = 32128` â€” Tamanho do vocabulÃ¡rio

O tokenizador **SentencePiece** (Kudo & Richardson, 2018) usa um modelo **unigram** que decompÃµe textos em subpalavras:

```
Texto: "randomized controlled trial" â†’ 32128 possÃ­veis subpalavras

TokenizaÃ§Ã£o:
  "randomized"     â†’ ["_random", "ized"]              (2 tokens)
  "controlled"     â†’ ["_control", "led"]               (2 tokens)
  "trial"          â†’ ["_trial"]                         (1 token)
  Total: 5 tokens

Texto raro: "bronchopneumonia" â†’ ["_broncho", "pne", "umon", "ia"] (4 tokens)
Texto comum: "the" â†’ ["_the"]  (1 token)
```

A embedding layer mapeia cada um dos 32.128 tokens para um vetor de $d_{model} = 768$ dimensÃµes:

$$E \in \mathbb{R}^{32128 \times 768}$$

Isso soma **24,7M parÃ¢metros** apenas na embedding (compartilhada entre encoder e decoder no T5).

**ReferÃªncia**: Kudo, T. & Richardson, J. (2018). *SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing*. In Proceedings of EMNLP 2018, pp. 66â€“71. https://aclanthology.org/D18-2012/

#### CÃ¡lculo do total de parÃ¢metros (~220M)

A contagem detalhada de parÃ¢metros do modelo T5-base:

$$\text{Params}_{total} = \text{Params}_{embedding} + \text{Params}_{encoder} + \text{Params}_{decoder} + \text{Params}_{head}$$

```
1. EMBEDDING (compartilhada encoder/decoder):
   E = vocab_size Ã— d_model = 32128 Ã— 768 = 24,674,304 params

2. ENCODER (12 camadas, cada uma com):
   a) Self-Attention (sem bias â€” T5 usa projeÃ§Ãµes lineares sem termo de viÃ©s):
      W_Q, W_K, W_V: 3 Ã— (d_model Ã— d_kv Ã— num_heads) = 3 Ã— (768 Ã— 64 Ã— 12) = 1,769,472
      W_O:           d_model Ã— d_model = 768 Ã— 768 = 589,824
      T5LayerNorm:   d_model = 768  (apenas scale, sem bias â€” RMSNorm)
      Subtotal attn: 2,360,064 /camada

   b) Feed-Forward (sem bias nas camadas lineares):
      Wâ‚: d_model Ã— d_ff = 768 Ã— 3072 = 2,359,296
      Wâ‚‚: d_ff Ã— d_model = 3072 Ã— 768 = 2,359,296
      T5LayerNorm: d_model = 768  (apenas scale)
      Subtotal FFN: 4,719,360 /camada

   + Relative Attention Bias (apenas no bloco 0, compartilhado):
      relative_attention_bias: num_buckets Ã— num_heads = 32 Ã— 12 = 384

   Total Encoder: 12 Ã— (2,360,064 + 4,719,360) + 384 + 768 (final LN)
   Total Encoder: 84,954,240

3. DECODER (12 camadas, cada uma com):
   a) Self-Attention:    mesma estrutura    = 2,360,064 /camada
   b) Cross-Attention:   mesma estrutura    = 2,360,064 /camada
   c) Feed-Forward:      mesma estrutura    = 4,719,360 /camada

   + Relative Attention Bias (bloco 0): 384
   Total Decoder: 12 Ã— (2,360,064 + 2,360,064 + 4,719,360) + 384 + 768 (final LN)
   Total Decoder: 113,275,008

4. LM HEAD (compartilha pesos com embedding):
   Sem parÃ¢metros adicionais (tied weights)

TOTAL: 24,674,304 + 84,954,240 + 113,275,008 = 222,903,552 â‰ˆ 223M âœ…
(Verificado: safetensors do modelo contÃ©m exatamente 222,903,552 parÃ¢metros)
```

### Fluxo completo Encoder-Decoder

```
ENCODER (processa o texto fonte em paralelo):

  Input: "The patient presented fever"
    â†“ Tokenize + Embed
  Xâ‚€ = [eâ‚, eâ‚‚, eâ‚ƒ, eâ‚„]  âˆˆ â„^(4Ã—768)   (4 tokens Ã— 768 dims)
    â†“ + Relative Position Bias
    â†“
  â”Œâ”€â”€â”€ Camada 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Layer Norm â†’ Self-Attention â†’ Residual Connection    â”‚
  â”‚ Xâ‚ = LayerNorm(Xâ‚€) â†’ MultiHead(Q,K,V) + Xâ‚€        â”‚
  â”‚ Layer Norm â†’ FFN â†’ Residual Connection               â”‚
  â”‚ Xâ‚ = LayerNorm(Xâ‚) â†’ FFN(Xâ‚) + Xâ‚                 â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ ... repete 12 vezes ...
  â”Œâ”€â”€â”€ Camada 12 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ (mesma estrutura)                                    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ Final Layer Norm
  H_enc = [hâ‚, hâ‚‚, hâ‚ƒ, hâ‚„]  âˆˆ â„^(4Ã—768)   â† "memÃ³ria" do encoder

DECODER (gera token por token, autoregressivamente):

  Target: "<pad> O paciente apresentou febre" (shifted right)
    â†“ Tokenize + Embed
  Yâ‚€ = [dâ‚, dâ‚‚, dâ‚ƒ, dâ‚„, dâ‚…]
    â†“
  â”Œâ”€â”€â”€ Camada 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Layer Norm â†’ Masked Self-Attention â†’ Residual        â”‚
  â”‚   (cada token sÃ³ vÃª tokens ANTERIORES â€” causal)      â”‚
  â”‚ Layer Norm â†’ Cross-Attention(Q=dec, K=enc, V=enc)    â”‚
  â”‚   (decoder "consulta" o encoder: alinha sourceâ†”target)â”‚
  â”‚ Layer Norm â†’ FFN â†’ Residual                          â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ ... repete 12 vezes ...
    â†“ Final Layer Norm
    â†“ LM Head (projeÃ§Ã£o linear â†’ logits âˆˆ â„^32128)
    â†“ Softmax â†’ probabilidade sobre todo o vocabulÃ¡rio
  P("O" | "The patient presented fever", <pad>) = 0.87
  P("paciente" | "The patient presented fever", <pad> O) = 0.92
  ...
```

### PrÃ©-treinamento e Dados Originais

- **PrÃ©-treinamento base**: PTT5 â€” modelo T5 prÃ©-treinado em corpus em portuguÃªs
- **Fine-tuning de traduÃ§Ã£o (pelos autores)**: ParaCrawl (5M+ pares EN-PT) + Corpora biomÃ©dica cientÃ­fica (6M+ pares)
- **Tarefa**: TraduÃ§Ã£o ENâ†’PT com prefixo `"translate English to Portuguese: "`
- **Tokenizador**: SentencePiece (unigram) com vocabulÃ¡rio de 32k tokens

### ReferÃªncia AcadÃªmica

```bibtex
@inproceedings{lopes-etal-2020-lite,
    title     = "Lite Training Strategies for {P}ortuguese-{E}nglish and {E}nglish-{P}ortuguese Translation",
    author    = "Lopes, Alexandre and Nogueira, Rodrigo and Lotufo, Roberto and Pedrini, Helio",
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month     = nov,
    year      = "2020",
    address   = "Online",
    publisher = "Association for Computational Linguistics",
    url       = "https://aclanthology.org/2020.wmt-1.90",
    pages     = "833--840",
}
```

### Como o modelo foi selecionado? â€” O caso Helsinki

A seleÃ§Ã£o do modelo nÃ£o foi automÃ¡tica. O `Helsinki-NLP/opus-mt-tc-big-en-pt` foi a **primeira escolha** para fine-tuning, pois liderou o ranking no STAGE 1 (BLEU=37.47, chrF=59.85 na avaliaÃ§Ã£o geral). PorÃ©m, o fine-tuning do Helsinki **fracassou** â€” os resultados **pioraram** em relaÃ§Ã£o ao modelo base.

#### Tentativa com Helsinki: configuraÃ§Ã£o e resultados

| ParÃ¢metro               | Helsinki (1Âª tentativa)       | Unicamp-T5 (2Âª tentativa)         |
|--------------------------|-------------------------------|-----------------------------------|
| Arquitetura              | MarianMT (~600M params)       | T5 (~220M params)                 |
| Dataset de treino        | 80.000 exemplos               | 18.000 exemplos                   |
| Dataset de validaÃ§Ã£o     | âŒ Nenhum                     | âœ… 2.000 exemplos                 |
| Epochs                   | 5                             | 12                                |
| Batch size               | 8                             | 8                                 |
| Gradient accumulation    | âŒ NÃ£o                        | âœ… 2 (effective batch = 16)       |
| Learning rate            | ~2e-5 (default)               | 1e-5 (conservador)               |
| FP16 (mixed precision)   | âŒ NÃ£o                        | âœ… Sim                            |
| max_seq_len              | âŒ NÃ£o configurado (default)  | âœ… 256 tokens                     |
| Early stopping           | âŒ NÃ£o                        | âœ… patience=2                     |

#### Por que o Helsinki fracassou?

```
Helsinki: Training Loss ao longo de 50.000 steps (5 epochs)

Loss
 8 â”¤ â–ˆâ–ˆ
 7 â”¤  â–ˆâ–ˆ
 6 â”¤    â–ˆâ–ˆâ–ˆ
 5 â”¤       â–ˆâ–ˆâ–ˆâ–ˆ
 4 â”¤           â–ˆâ–ˆâ–ˆâ–ˆ
 3 â”¤               â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
 2 â”¤                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
 1 â”¤                           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
 0 â”¤                                           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â† 0.14 (OVERFITTING!)
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   0     10k    20k    30k    40k    50k steps
```

AnÃ¡lise do `trainer_state.json` do Helsinki:
- **Training loss**: 7.65 â†’ 0.14 (queda de 98%) â€” o modelo **memorizou** os dados de treino
- **Eval loss**: **inexistente** â€” nenhuma avaliaÃ§Ã£o durante o treino (0 eval entries)
- **best_metric**: `None` â€” sem monitoramento, sem seleÃ§Ã£o do melhor checkpoint
- **Resultado final**: BLEU = **36** (era 42.64 no SciELO base â†’ **degradaÃ§Ã£o de -6.6 pontos!**)
- **chrF** = **65** (era 68.93 â†’ **degradaÃ§Ã£o de -3.9 pontos**)
- **COMET e BERTScore**: nÃ£o foi possÃ­vel medir

O diagnÃ³stico Ã© claro: **catastrophic forgetting** (esquecimento catastrÃ³fico). Sem conjunto de validaÃ§Ã£o, sem early stopping, e sem regularizaÃ§Ã£o, o modelo com 600M de parÃ¢metros **memorizou** os 80k exemplos de treino (loss â†’ 0.14) mas **perdeu a capacidade de generalizar** para textos novos. Este Ã© um fenÃ´meno bem documentado na literatura de adaptaÃ§Ã£o de domÃ­nio em NMT (Miceli Barone et al., 2017; Freitag & Al-Onaizan, 2016).

#### Por que o Unicamp-T5 teve sucesso?

A segunda tentativa aplicou todas as liÃ§Ãµes aprendidas com a falha do Helsinki:

1. **Conjunto de validaÃ§Ã£o (2k exemplos)**: Permitiu monitorar eval_loss a cada epoch e detectar overfitting
2. **Early stopping (patience=2)**: Interromperia o treino automaticamente se eval_loss parasse de melhorar
3. **Gradient accumulation (2)**: Effective batch size de 16, suavizando gradientes ruidosos
4. **Learning rate conservador (1e-5)**: Metade do default, evitando atualizaÃ§Ãµes destrutivas
5. **FP16 (mixed precision)**: Viabilizou treinar na RTX 4050 (6GB VRAM) sem out-of-memory
6. **max_seq_len=256**: Truncamento explÃ­cito, evitando sequÃªncias variÃ¡veis que desestabilizam o treino
7. **Modelo 3x menor (220M vs 600M)**: Menos propenso a overfitting com dados limitados

**Resultado**: Training loss convergiu para **0.97** â€” praticamente igual ao eval_loss (**0.97**), indicando zero overfitting. BLEU subiu de 40.06 para **45.51** (+13.6%).

#### FundamentaÃ§Ã£o: por que menos dados + mais tÃ©cnicas supera mais dados sem tÃ©cnicas?

A literatura de adaptaÃ§Ã£o de domÃ­nio em NMT sustenta fortemente este resultado:

- **Miceli Barone et al. (2017)** demonstraram que, ao fazer fine-tuning de NMT em dados in-domain de tamanho limitado, **tÃ©cnicas de regularizaÃ§Ã£o** (dropout, L2, early stopping) sÃ£o mais importantes que o volume de dados. Sem regularizaÃ§Ã£o, modelos grandes overfitam rapidamente, mesmo com datasets grandes. O artigo encontra uma relaÃ§Ã£o **logarÃ­tmica** entre volume de dados e ganho em BLEU â€” ou seja, dobrar os dados nÃ£o dobra a qualidade.

- **Freitag & Al-Onaizan (2016)** mostraram que Ã© possÃ­vel adaptar modelos NMT a novos domÃ­nios **com poucos dados in-domain**, desde que o processo de fine-tuning seja controlado. A chave Ã© **qualidade do processo**, nÃ£o quantidade de dados.

- **Neubig & Hu (2018)** propuseram "similar-language regularization" para evitar overfitting em adaptaÃ§Ã£o com dados limitados, confirmando que a **prevenÃ§Ã£o de overfitting** Ã© o fator crÃ­tico em domain adaptation.

- **Koehn & Knowles (2017)** identificaram 6 desafios para NMT, incluindo que modelos neurais sÃ£o particularmente sensÃ­veis a **dados fora do domÃ­nio** e que adaptaÃ§Ã£o de domÃ­nio requer tÃ©cnicas cuidadosas.

No nosso caso, os 18k exemplos do SciELO sÃ£o **altamente representativos** do domÃ­nio-alvo (abstracts cientÃ­ficos biomÃ©dicos ENâ†’PT), enquanto os 80k do Helsinki possivelmente continham ruÃ­do ou distribuiÃ§Ã£o menos focada. Mais epochs (12 vs 5) permitiram **exposiÃ§Ã£o repetida ao vocabulÃ¡rio especializado** do domÃ­nio, enquanto o early stopping impediu que essa repetiÃ§Ã£o causasse memorizaÃ§Ã£o.

```
RESUMO DA SELEÃ‡ÃƒO:

Helsinki (1Âª tentativa)         Unicamp-T5 (2Âª tentativa)
â”œâ”€ 600M params                  â”œâ”€ 220M params
â”œâ”€ 80k treino, 0 validaÃ§Ã£o      â”œâ”€ 18k treino, 2k validaÃ§Ã£o
â”œâ”€ 5 epochs, sem early stop     â”œâ”€ 12 epochs, early stopping
â”œâ”€ Sem grad_accum, sem fp16     â”œâ”€ grad_accum=2, fp16
â”œâ”€ Loss: 7.65 â†’ 0.14 âš ï¸        â”œâ”€ Loss: ~2.5 â†’ 0.97 âœ…
â”œâ”€ BLEU: 42.64 â†’ 36 ğŸ“‰ (-15.6%) â”œâ”€ BLEU: 40.06 â†’ 45.51 ğŸ“ˆ (+13.6%)
â””â”€ FRACASSO (overfitting)       â””â”€ SUCESSO (generalizaÃ§Ã£o)
```

---

## Pipeline de 5 EstÃ¡gios

```
STAGE 1: AVALIAÃ‡ÃƒO INICIAL
â”œâ”€ Testar 6 modelos prÃ©-treinados em 3 datasets pÃºblicos
â”œâ”€ Calcular BLEU, chrF, COMET, BERTScore
â””â”€ Resultado: evaluation_results/translation_metrics_all.csv
        â†“
STAGE 2: SELEÃ‡ÃƒO DO MODELO
â”œâ”€ 1Âª tentativa: Helsinki (fracasso â€” catastrophic forgetting)
â”œâ”€ 2Âª tentativa: unicamp-dl/translation-en-pt-t5 (sucesso)
â””â”€ Resultado: modelo definido com base em experimentaÃ§Ã£o empÃ­rica
        â†“
STAGE 3: PREPARAÃ‡ÃƒO DE DADOS
â”œâ”€ Separar SciELO em 3 splits nÃ£o-sobrepostos:
â”‚   â”œâ”€ 18.000 exemplos para TREINO
â”‚   â”œâ”€  2.000 exemplos para VALIDAÃ‡ÃƒO (early stopping)
â”‚   â””â”€  5.000 exemplos para TESTE
â””â”€ Resultado: finetuning/abstracts-datasets/*.csv
        â†“
STAGE 4: FINE-TUNING
â”œâ”€ GPU: NVIDIA RTX 4050 (6GB VRAM)
â”œâ”€ 12 epochs, batch_size=8, grad_accum=2, lr=1e-5
â”œâ”€ Early stopping com patience=2
â””â”€ Resultado: unicamp-t5/unicamp-t5/ (modelo fine-tuned)
        â†“
STAGE 5: AVALIAÃ‡ÃƒO FINAL
â”œâ”€ Testar modelo base vs fine-tuned nos MESMOS 5k dados de teste
â”œâ”€ Calcular delta de mÃ©tricas
â””â”€ Resultado: scielo_before_finetuning.csv / scielo_after_finetuning_epoch_*.csv
```

---

## STAGE 1: AvaliaÃ§Ã£o Inicial dos Modelos

### Objetivo
Avaliar 6 modelos prÃ©-treinados em 3 datasets pÃºblicos para estabelecer baselines.

### Modelos Avaliados

| # | Modelo                                                        | Arquitetura | ParÃ¢metros |
|---|---------------------------------------------------------------|-------------|------------|
| 1 | `Helsinki-NLP/opus-mt-tc-big-en-pt`                          | MarianMT    | ~600M      |
| 2 | `Narrativa/mbart-large-50-finetuned-opus-en-pt-translation`  | mBART-50    | ~611M      |
| 3 | `unicamp-dl/translation-en-pt-t5`                            | T5          | ~220M      |
| 4 | `VanessaSchenkel/unicamp-finetuned-en-to-pt-dataset-ted`     | T5          | ~220M      |
| 5 | `danhsf/m2m100_418M-finetuned-kde4-en-to-pt_BR`             | M2M100      | ~418M      |
| 6 | `quickmt/quickmt-en-pt`                                      | CTranslate2 | â€”          |

### Datasets PÃºblicos

| Dataset      | Exemplos | DescriÃ§Ã£o                    |
|--------------|----------|------------------------------|
| WMT24++      | 998      | AvaliaÃ§Ã£o enâ†’pt_BR           |
| ParaCrawl    | 5.000    | Crawl web paralelo enâ†’pt     |
| Flores       | 1.012    | Facebook multilingual        |

### MÃ©tricas

| MÃ©trica       | Tipo       | Range | DescriÃ§Ã£o                                          |
|---------------|------------|-------|----------------------------------------------------|
| **BLEU**      | N-gramas   | 0-100 | PrecisÃ£o de n-gramas (1-4) com brevity penalty     |
| **chrF**      | Caracteres | 0-100 | F-score baseado em caracteres                      |
| **COMET**     | Neural     | 0-1   | Score neural aprendido (Unbabel/wmt22-comet-da)    |
| **BERTScore** | Neural     | 0-1   | Similaridade semÃ¢ntica via embeddings BERT         |

### Resultados â€” MÃ©dia por Modelo (3 datasets)

| #  | Modelo          | BLEU  | chrF  | COMET  | BERTScore | GPU (MB) |
|----|-----------------|------:|------:|-------:|----------:|---------:|
| 1  | Helsinki        | 37.47 | 59.85 | 0.8250 | 0.8667    | 904      |
| 2  | Narrativa mBART | 21.01 | 40.27 | 0.7572 | 0.8350    | 2.340    |
| 3  | Unicamp-T5      | 14.58 | 32.41 | 0.6670 | 0.7922    | 859      |
| 4  | VanessaSchenkel | 8.52  | 25.34 | 0.6342 | 0.7862    | 859      |
| 5  | M2M100          | 22.08 | 48.21 | 0.7530 | 0.8333    | 1.863    |
| 6  | QuickMT         | 0.00  | 4.17  | 0.2701 | 0.4754    | 9        |

### Resultados Detalhados â€” Por Dataset

**WMT24++ (998 exemplos, sentenÃ§as longas ~33 palavras/sentenÃ§a)**

| Modelo          | BLEU  | chrF  | COMET  | BERTScore | Tempo       |
|-----------------|------:|------:|-------:|----------:|------------:|
| Helsinki        | 33.71 | 58.86 | 0.7825 | 0.8622    | 529s        |
| Narrativa mBART | 6.54  | 25.48 | 0.6452 | 0.7917    | 797s        |
| Unicamp-T5      | 3.55  | 19.73 | 0.5391 | 0.7573    | 237s        |
| VanessaSchenkel | 2.77  | 17.19 | 0.5091 | 0.7562    | 215s        |
| M2M100          | 22.99 | 50.08 | 0.7012 | 0.8404    | 888s        |
| QuickMT         | 0.00  | 4.80  | 0.2480 | 0.4871    | 59s         |

**ParaCrawl (5.000 exemplos, sentenÃ§as curtas ~7 palavras/sentenÃ§a)**

| Modelo          | BLEU  | chrF  | COMET  | BERTScore | Tempo       |
|-----------------|------:|------:|-------:|----------:|------------:|
| Helsinki        | 39.63 | 59.98 | 0.8452 | 0.8696    | 740s        |
| Narrativa mBART | 27.07 | 46.75 | 0.8083 | 0.8544    | 2.013s      |
| Unicamp-T5      | 19.46 | 37.99 | 0.7239 | 0.8076    | 633s        |
| VanessaSchenkel | 11.05 | 28.89 | 0.6868 | 0.7992    | 610s        |
| M2M100          | 22.41 | 47.11 | 0.7735 | 0.8293    | 585s        |
| QuickMT         | 0.00  | 4.03  | 0.2789 | 0.4703    | 288s        |

**Flores (1.012 exemplos)**

| Modelo          | BLEU  | chrF  | COMET  | BERTScore | Tempo       |
|-----------------|------:|------:|-------:|----------:|------------:|
| Helsinki        | 39.08 | 60.72 | 0.8473 | 0.8683    | 131s        |
| Narrativa mBART | 29.43 | 48.59 | 0.8182 | 0.8588    | 378s        |
| Unicamp-T5      | 20.72 | 39.52 | 0.7380 | 0.8116    | 122s        |
| VanessaSchenkel | 11.74 | 29.93 | 0.7066 | 0.8032    | 111s        |
| M2M100          | 20.85 | 47.45 | 0.7842 | 0.8301    | 247s        |
| QuickMT         | 0.00  | 3.68  | 0.2835 | 0.4689    | 59s         |

### Comandos

```bash
# Avaliar 5 modelos primÃ¡rios
python models-test.py --full

# Avaliar 6Âº modelo (QuickMT - CTranslate2)
python evaluate_quickmt.py --full

# Retomar avaliaÃ§Ã£o interrompida
python models-test.py --resume
python evaluate_quickmt.py --resume
```

### SaÃ­da
- `evaluation_results/translation_metrics_all.csv` â€” consolidado
- `evaluation_results/<modelo>.csv` â€” individual por modelo

---

## STAGE 2: SeleÃ§Ã£o do Modelo

### Objetivo
Selecionar o melhor modelo para fine-tuning por experimentaÃ§Ã£o prÃ¡tica.

### Processo Real de SeleÃ§Ã£o

A seleÃ§Ã£o nÃ£o foi automÃ¡tica por score composto. Foi um processo **empÃ­rico em duas etapas**:

**Etapa 1 â€” Helsinki (fracasso)**:
O modelo com melhor desempenho no STAGE 1 (Helsinki, BLEU=37.47) foi a escolha natural. Foi feito fine-tuning com 80k exemplos, 5 epochs, batch_size=8, sem validaÃ§Ã£o, sem early stopping, sem gradient accumulation, sem fp16, sem controle de max_seq_len. O resultado foi **catastrophic forgetting**: BLEU caiu de 42.64â†’36 no SciELO, chrF de 68.93â†’65. O modelo memorizou o treino (lossâ†’0.14) mas perdeu generalizaÃ§Ã£o.

**Etapa 2 â€” Unicamp-T5 (sucesso)**:
Com as liÃ§Ãµes aprendidas, a segunda tentativa usou o `unicamp-dl/translation-en-pt-t5` (220M params, 3x menor), com todas as tÃ©cnicas de regularizaÃ§Ã£o: validaÃ§Ã£o (2k), early stopping, gradient accumulation, fp16, max_seq_len=256, lr conservador. BLEU subiu de 40.06â†’45.51 (+13.6%).

### Score Composto (ferramenta auxiliar)
O script `choose_best_model.py` calcula um score composto para referÃªncia:

$$S = 0.30 \cdot \hat{B} + 0.25 \cdot \hat{C}_r + 0.25 \cdot \hat{C}_o + 0.20 \cdot \hat{B}_s$$

Onde cada mÃ©trica Ã© normalizada min-max para $[0, 1]$ entre os modelos avaliados:

$$\hat{x} = \frac{x - x_{\min}}{x_{\max} - x_{\min}}$$

```
Exemplo: normalizaÃ§Ã£o do BLEU
  Valores brutos: Helsinki=37.47, Narrativa=21.01, Unicamp-T5=14.58, ...
  min = 0.00 (QuickMT), max = 37.47 (Helsinki)
  
  BLEU_norm(Helsinki)  = (37.47 - 0.00) / (37.47 - 0.00) = 1.000
  BLEU_norm(Unicamp-T5) = (14.58 - 0.00) / (37.47 - 0.00) = 0.389
```

**Pesos**: BLEU recebe maior peso (0.30) por ser a mÃ©trica mais estabelecida. chrF e COMET dividem 0.25 cada. BERTScore recebe 0.20 por ter menor correlaÃ§Ã£o com traduÃ§Ã£o especificamente.

### Comando
```bash
python choose_best_model.py
```

### Resultado
Modelo selecionado: **`unicamp-dl/translation-en-pt-t5`** â€” definido apÃ³s a falha empÃ­rica do Helsinki, validado por sua eficiÃªncia computacional (220M params, RTX 4050 compatÃ­vel) e pela qualidade dos resultados de fine-tuning (+5.45 BLEU).

---

## STAGE 3: PreparaÃ§Ã£o de Dados SciELO

### Objetivo
Criar 3 splits nÃ£o-sobrepostos do dataset SciELO (2.7M exemplos totais).

### DivisÃ£o dos Dados

| Split      | Exemplos | Uso                                    |
|------------|----------|----------------------------------------|
| Treino     | 18.000   | Fine-tuning do modelo                  |
| ValidaÃ§Ã£o  | 2.000    | Monitorar convergÃªncia + early stopping|
| Teste      | 5.000    | AvaliaÃ§Ã£o final (mesmos para base e fine-tuned) |

**Total: 25.000 exemplos (~0.9% do corpus completo)**

### Justificativa do Dataset Compacto

- **18k treino**: Suficiente para adaptaÃ§Ã£o de domÃ­nio (abstracts cientÃ­ficos) sem overfitting
- **2k validaÃ§Ã£o**: Monitora eval_loss por epoch e aciona early stopping
- **5k teste**: Mesmo conjunto usado na avaliaÃ§Ã£o do modelo base, garantindo comparaÃ§Ã£o justa
- **Seed fixo (42)**: Splits sÃ£o determinÃ­sticos e reprodutÃ­veis

### Como funcionam os 2.000 exemplos de validaÃ§Ã£o?

O conjunto de validaÃ§Ã£o **nÃ£o Ã© usado para treinar** o modelo â€” seus pesos nunca sÃ£o atualizados com base nesses dados. Ele serve exclusivamente para **monitorar a generalizaÃ§Ã£o** durante o treino:

```
Fluxo por epoch:

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ TREINO (18k exemplos)                                        â”‚
  â”‚  O modelo processa todos os 18k exemplos em mini-batches     â”‚
  â”‚  de 8, atualizando pesos a cada batch (gradient descent).    â”‚
  â”‚  â†’ Calcula: training_loss (quÃ£o bem acerta os dados de treino)â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“ (ao final de cada epoch)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ VALIDAÃ‡ÃƒO (2k exemplos) â€” modo inference, SEM gradient       â”‚
  â”‚  O modelo traduz os 2k exemplos SEM atualizar pesos.         â”‚
  â”‚  â†’ Calcula: eval_loss (quÃ£o bem acerta dados NUNCA vistos)   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ DECISÃƒO DO EARLY STOPPING                                    â”‚
  â”‚  Se eval_loss melhorou â†’ salva checkpoint, reseta contador   â”‚
  â”‚  Se eval_loss NÃƒO melhorou por 2 epochs â†’ PARA o treino     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Por que isso importa?** No caso do Helsinki (sem validaÃ§Ã£o), o treino rodou todos os 50k steps cegamente. A training loss caiu para 0.14 (parecia excelente!), mas o modelo estava memorizando dados â€” sem eval_loss, nÃ£o havia como detectar a degradaÃ§Ã£o. Com validaÃ§Ã£o, se a eval_loss comeÃ§asse a subir (sinal de overfitting), o early stopping interromperia o treino antes do dano.

| CenÃ¡rio                          | train_loss | eval_loss | DiagnÃ³stico        |
|----------------------------------|:----------:|:---------:|:-------------------|
| Helsinki (sem validaÃ§Ã£o)         | 0.14       | âŒ N/A    | Overfitting oculto |
| Unicamp-T5 (com validaÃ§Ã£o)       | 0.97       | 0.97      | GeneralizaÃ§Ã£o ok   |
| Overfitting tÃ­pico (hipotÃ©tico)  | 0.10       | 2.50      | âš ï¸ PARAR treino    |

### Comandos

```bash
# PreparaÃ§Ã£o automÃ¡tica (integrada ao select_and_test_models.py)
python finetuning/select_and_test_models.py

# Ou manualmente
python -c "
from finetuning import config, data_utils
data_utils.prepare_evaluation_csv(
    abstracts_file='abstracts_scielo.csv',
    train_csv=config.SCIELO_TRAIN_CSV,
    val_csv=config.SCIELO_VAL_CSV,
    test_csv=config.SCIELO_TEST_CSV,
    train_samples=18_000,
    val_samples=2_000,
    test_samples=5_000
)
"
```

### SaÃ­da
```
finetuning/abstracts-datasets/
â”œâ”€â”€ scielo_abstracts_train.csv   (18.000 exemplos)
â”œâ”€â”€ scielo_abstracts_val.csv     ( 2.000 exemplos)
â””â”€â”€ scielo_abstracts_test.csv    ( 5.000 exemplos)
```

---

## STAGE 4: Fine-Tuning

### Objetivo
Fine-tunar o modelo `unicamp-dl/translation-en-pt-t5` no domÃ­nio de abstracts cientÃ­ficos.

### ConfiguraÃ§Ã£o de Treinamento

| ParÃ¢metro                  | Valor                  |
|----------------------------|------------------------|
| GPU                        | NVIDIA RTX 4050 (6GB)  |
| Epochs                     | 12                     |
| Batch size                 | 8                      |
| Gradient accumulation      | 2                      |
| **Batch efetivo**          | **16**                 |
| Learning rate              | 1e-5                   |
| Warmup steps               | 500                    |
| Weight decay               | 0.01                   |
| Max sequence length        | 256 tokens             |
| PrecisÃ£o                   | FP16 (mixed precision) |
| Otimizador                 | AdamW                  |
| Early stopping patience    | 2 epochs               |
| Gradient checkpointing     | Ativado                |
| Steps por epoch            | 1.125                  |
| Save strategy              | Por epoch              |
| Seed                       | 42                     |

### ConfiguraÃ§Ã£o do Modelo â€” `config.json` (antes vs depois)

A arquitetura do modelo **nÃ£o muda** durante o fine-tuning â€” apenas os pesos sÃ£o atualizados. As diferenÃ§as no `config.json` sÃ£o campos de metadados adicionados pela versÃ£o mais recente do `transformers`.

#### Modelo Original (HuggingFace)

```json
{
  "_name_or_path": "./",
  "architectures": ["T5ForConditionalGeneration"],
  "d_ff": 3072,
  "d_kv": 64,
  "d_model": 768,
  "decoder_start_token_id": 0,
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "relu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "n_positions": 512,
  "num_decoder_layers": 12,
  "num_heads": 12,
  "num_layers": 12,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_num_buckets": 32,
  "torch_dtype": "float32",
  "transformers_version": "4.11.3",
  "use_cache": true,
  "vocab_size": 32128
}
```

#### Modelo Fine-tuned (local)

```json
{
  "architectures": ["T5ForConditionalGeneration"],
  "classifier_dropout": 0.0,
  "d_ff": 3072,
  "d_kv": 64,
  "d_model": 768,
  "decoder_start_token_id": 0,
  "dense_act_fn": "relu",
  "dropout_rate": 0.1,
  "dtype": "float32",
  "eos_token_id": 1,
  "feed_forward_proj": "relu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": false,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "n_positions": 512,
  "num_decoder_layers": 12,
  "num_heads": 12,
  "num_layers": 12,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "transformers_version": "4.57.6",
  "use_cache": true,
  "vocab_size": 32128
}
```

#### DiferenÃ§as

| Campo                          | Original (HF)   | Fine-tuned (local) | ObservaÃ§Ã£o                          |
|--------------------------------|------------------|---------------------|-------------------------------------|
| `_name_or_path`                | `"./"`          | *(removido)*        | Caminho local do autor original     |
| `torch_dtype` / `dtype`        | `"float32"`     | `"float32"`        | Apenas renomeaÃ§Ã£o de campo          |
| `transformers_version`         | `4.11.3`         | `4.57.6`            | VersÃ£o da lib no momento do salvamento |
| `classifier_dropout`           | *(ausente)*      | `0.0`               | Adicionado pela versÃ£o nova         |
| `dense_act_fn`                 | *(ausente)*      | `"relu"`           | ExplicitaÃ§Ã£o da ativaÃ§Ã£o            |
| `is_gated_act`                 | *(ausente)*      | `false`             | T5 padrÃ£o nÃ£o usa gated activation  |
| `relative_attention_max_distance`| *(ausente)*    | `128`               | Default explicitado pela versÃ£o nova |

> **Nota**: Todos os hiperparÃ¢metros arquiteturais (d_model, d_ff, num_layers, num_heads, vocab_size) sÃ£o **idÃªnticos**. O fine-tuning altera **apenas os pesos** (`model.safetensors`), nÃ£o a arquitetura.

### ConfiguraÃ§Ã£o de GeraÃ§Ã£o â€” `generation_config.json`

Arquivo criado automaticamente pelo `Seq2SeqTrainer` (nÃ£o existia no modelo original do HuggingFace):

```json
{
  "_from_model_config": true,
  "decoder_start_token_id": 0,
  "eos_token_id": [1],
  "pad_token_id": 0,
  "transformers_version": "4.57.6"
}
```

| ParÃ¢metro              | Valor | DescriÃ§Ã£o                                         |
|------------------------|-------|---------------------------------------------------|
| `decoder_start_token_id` | 0   | Token `<pad>` usado para iniciar a decodificaÃ§Ã£o  |
| `eos_token_id`           | 1   | Token `</s>` marca fim da sequÃªncia gerada        |
| `pad_token_id`           | 0   | Token `<pad>` para padding                        |

### Argumentos de Treinamento â€” `Seq2SeqTrainingArguments`

ConfiguraÃ§Ã£o completa passada ao `Seq2SeqTrainer` (de `finetuning/trainer.py`):

```python
Seq2SeqTrainingArguments(
    output_dir="./models/finetuned-scielo/unicamp-t5",
    overwrite_output_dir=False,
    num_train_epochs=12,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=1e-5,
    warmup_steps=500,
    weight_decay=0.01,
    save_strategy="epoch",
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    gradient_accumulation_steps=2,
    fp16=True,
    logging_steps=100,
    predict_with_generate=True,
    optim="adamw_torch",
    seed=42,
    report_to=[],
    eval_strategy="epoch",
)
```

| Argumento                   | Valor                          | Finalidade                                    |
|-----------------------------|--------------------------------|-----------------------------------------------|
| `output_dir`                | `./models/finetuned-scielo/unicamp-t5` | DiretÃ³rio de saÃ­da dos checkpoints     |
| `overwrite_output_dir`      | `False`                        | Preserva checkpoints existentes               |
| `num_train_epochs`          | 12                             | NÃºmero total de epochs                        |
| `per_device_train_batch_size`| 8                             | Batch size por GPU                            |
| `learning_rate`             | 1e-5                           | Taxa de aprendizado (linear warmup + decay)   |
| `warmup_steps`              | 500                            | Steps de warmup linear do LR                  |
| `weight_decay`              | 0.01                           | RegularizaÃ§Ã£o L2 desacoplada (AdamW)          |
| `save_strategy`             | `"epoch"`                     | Salva checkpoint a cada epoch                 |
| `save_total_limit`          | 2                              | MantÃ©m apenas os 2 Ãºltimos checkpoints        |
| `load_best_model_at_end`    | `True`                         | Carrega melhor modelo (menor eval_loss) ao final |
| `metric_for_best_model`     | `"eval_loss"`                 | MÃ©trica para selecionar melhor checkpoint     |
| `gradient_accumulation_steps`| 2                             | Acumula gradientes de 2 mini-batches          |
| `fp16`                      | `True`                         | Mixed precision (Tensor Cores da RTX 4050)    |
| `logging_steps`             | 100                            | Log de mÃ©tricas a cada 100 steps              |
| `predict_with_generate`     | `True`                         | Usa `model.generate()` para avaliaÃ§Ã£o         |
| `optim`                     | `"adamw_torch"`               | Otimizador AdamW nativo do PyTorch            |
| `seed`                      | 42                             | Seed para reprodutibilidade                   |
| `eval_strategy`             | `"epoch"`                     | Avalia no dataset de validaÃ§Ã£o a cada epoch   |

### Comando Executado

```bash
python finetuning/finetune_selected_models.py \
  --model unicamp-t5 \
  --epochs 12 \
  --batch_size 8 \
  --grad_accum_steps 2 \
  --lr 1e-5 \
  --fp16 \
  --max_seq_len 256 \
  --early_stopping_patience 2 \
  --skip_prepare
```

### ExplicaÃ§Ã£o Detalhada dos ParÃ¢metros

Cada parÃ¢metro do comando foi escolhido para maximizar a qualidade do fine-tuning dentro das restriÃ§Ãµes de hardware (RTX 4050, 6GB VRAM). Abaixo, a explicaÃ§Ã£o tÃ©cnica de cada um com exemplos visuais.

---

#### `--model unicamp-t5`

Seleciona o modelo `unicamp-dl/translation-en-pt-t5` do dicionÃ¡rio `config.MODELS`. Veja a seÃ§Ã£o [STAGE 2](#stage-2-seleÃ§Ã£o-do-modelo) para justificativa da seleÃ§Ã£o.

---

#### `--epochs 12`

**O que Ã©**: NÃºmero de passagens completas pelo dataset de treino (18.000 exemplos).

**ReferÃªncia**: Smith, L. N. (2018). *A disciplined approach to neural network hyper-parameters: Part 1 â€“ learning rate, batch size, momentum, and weight decay*. arXiv:1803.09820. https://arxiv.org/abs/1803.09820

**Por que 12**: O nÃºmero de epochs Ã© determinado pela convergÃªncia observada. A eval_loss continuou melhorando em todas as 12 epochs (0.973 no epoch 12), sem acionar early stopping. Mais epochs nÃ£o foram testados porque a taxa de melhoria nos Ãºltimos epochs era marginal (~0.0003/epoch).

```
Epoch 1  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  eval_loss: 1.0068
Epoch 2  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   eval_loss: 0.9931  â†“ 0.0137
Epoch 3  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    eval_loss: 0.9861  â†“ 0.0070
Epoch 4  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     eval_loss: 0.9818  â†“ 0.0043
Epoch 5  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      eval_loss: 0.9792  â†“ 0.0026
Epoch 6  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       eval_loss: 0.9772  â†“ 0.0020
Epoch 7  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        eval_loss: 0.9757  â†“ 0.0015
Epoch 8  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         eval_loss: 0.9747  â†“ 0.0010
Epoch 9  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          eval_loss: 0.9737  â†“ 0.0010
Epoch 10 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           eval_loss: 0.9733  â†“ 0.0004
Epoch 11 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            eval_loss: 0.9730  â†“ 0.0003
Epoch 12 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             eval_loss: 0.9730  â†“ 0.0001 â­

â†’ ReduÃ§Ã£o total: 0.0338 (3.36%)
â†’ 90% da melhoria ocorre nos primeiros 5 epochs
â†’ Epochs 10-12: rendimento decrescente (<0.001/epoch)
```

**Trade-off**: Poucas epochs = underfitting (modelo nÃ£o adaptado ao domÃ­nio). Muitas epochs = overfitting (modelo memoriza exemplos de treino). Com 12 epochs, train_loss (0.97) â‰ˆ eval_loss (0.97), indicando ausÃªncia de overfitting.

---

#### `--batch_size 8`

**O que Ã©**: NÃºmero de exemplos processados **simultaneamente** em cada forward pass pela GPU.

**ReferÃªncia**: Masters, D. & Luschi, C. (2018). *Revisiting Small Batch Training for Deep Neural Networks*. arXiv:1804.07612. https://arxiv.org/abs/1804.07612

**Por que 8 (e nÃ£o mais)**: LimitaÃ§Ã£o direta da VRAM da RTX 4050 (6GB). Com FP16, gradient checkpointing e max_seq_len=256:

```
MemÃ³ria GPU por batch (estimativa):

  Pesos do modelo (FP16):    ~440 MB  (220M params Ã— 2 bytes)
  AtivaÃ§Ãµes Encoder (FP16):  ~384 MB  (batch=8 Ã— 256 tokens Ã— 768 dim Ã— 12 layers)
  AtivaÃ§Ãµes Decoder (FP16):  ~384 MB  (idem)
  Gradientes (FP32):         ~880 MB  (220M params Ã— 4 bytes, mixed precision)
  Estados do otimizador:     ~1760 MB (AdamW: 2 estados Ã— 220M Ã— 4 bytes)
  Overhead CUDA:             ~200 MB
                             â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total estimado:           ~4048 MB (~4 GB)

  VRAM disponÃ­vel:           6144 MB (6 GB)
  Margem:                    ~2096 MB (suficiente âœ…)

  Com batch_size=16:         +768 MB ativaÃ§Ãµes â†’ ~4816 MB (ainda cabe, mas ajustado)
  Com batch_size=32:         +1536 MB ativaÃ§Ãµes â†’ OOM âŒ (Out of Memory)

â†’ batch_size=8 garante estabilidade com margem confortÃ¡vel
```

**Efeito no ruÃ­do do gradiente**:

```
Batch size pequeno (ex: 1-4):
  Gradiente â† âˆ‡L(xâ‚)                         â† Muito ruidoso
  â†’ ConvergÃªncia instÃ¡vel, LR precisa ser menor

Batch size mÃ©dio (ex: 8-16):
  Gradiente â† Â¼ Ã— (âˆ‡L(xâ‚) + âˆ‡L(xâ‚‚) + ... + âˆ‡L(xâ‚ˆ))  â† Bom equilÃ­brio
  â†’ Gradiente suavizado, convergÃªncia estÃ¡vel

Batch size grande (ex: 128-512):
  Gradiente â† 1/128 Ã— Î£ âˆ‡L(xáµ¢)               â† Muito suave
  â†’ ConvergÃªncia rÃ¡pida mas generalizaÃ§Ã£o pior
    (Sharp minima, referÃªncia: Keskar et al., 2017)
```

**ReferÃªncia**: Keskar, N. S. et al. (2017). *On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima*. In ICLR 2017. https://arxiv.org/abs/1609.04836

---

#### `--grad_accum_steps 2` â­

**O que Ã©**: **Gradient Accumulation** â€” acumula gradientes de mÃºltiplos mini-batches antes de atualizar os pesos. Simula um batch maior sem exigir mais VRAM.

**ReferÃªncia**: Ott, M. et al. (2018). *Scaling Neural Machine Translation*. In Proceedings of the Third Conference on Machine Translation (WMT), pp. 1â€“9. https://aclanthology.org/W18-6301/

**Batch Efetivo**:

$$\text{Batch efetivo} = \text{batch\_size} \times \text{grad\_accum\_steps} = 8 \times 2 = 16$$

**Funcionamento visual**:

```
SEM gradient accumulation (batch_size=16, se coubesse na VRAM):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Forward: 16 exemplos â†’ Loss â†’ Backward â†’ âˆ‡W â†’ Atualiza  â”‚
â”‚ VRAM: ~5.5 GB (pode dar OOM)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

COM gradient accumulation (batch_size=8, grad_accum=2):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: Forward 8 exemplos â†’ Lossâ‚ â†’ Backward â†’ âˆ‡Wâ‚     â”‚
â”‚         (NÃƒO atualiza pesos, apenas acumula gradiente)   â”‚
â”‚         VRAM: ~4 GB âœ…                                    â”‚
â”‚                                                          â”‚
â”‚ Step 2: Forward 8 exemplos â†’ Lossâ‚‚ â†’ Backward â†’ âˆ‡Wâ‚‚     â”‚
â”‚         âˆ‡W_total = âˆ‡Wâ‚ + âˆ‡Wâ‚‚                            â”‚
â”‚         Optimizer.step() â†’ Atualiza pesos com âˆ‡W_total   â”‚
â”‚         VRAM: ~4 GB âœ…                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Resultado MATEMÃTICO: Gradiente idÃªntico ao batch_size=16
Resultado PRÃTICO:    Metade da VRAM necessÃ¡ria
Custo:                ~2x mais lento (2 forward passes vs 1)
```

**Por que 2 e nÃ£o mais?**

```
grad_accum=1  â†’ batch efetivo = 8   â†’ gradiente ruidoso, convergÃªncia instÃ¡vel
grad_accum=2  â†’ batch efetivo = 16  â†’ bom equilÃ­brio ruÃ­do/estabilidade âœ…
grad_accum=4  â†’ batch efetivo = 32  â†’ mais estÃ¡vel, mas 4x mais lento
grad_accum=8  â†’ batch efetivo = 64  â†’ overkill para 18k exemplos (apenas 281 steps/epoch)

Steps por epoch com cada configuraÃ§Ã£o:
  grad_accum=1: 18000 / 8  = 2250 steps/epoch
  grad_accum=2: 18000 / 16 = 1125 steps/epoch  â† Nosso caso
  grad_accum=4: 18000 / 32 =  562 steps/epoch
  grad_accum=8: 18000 / 64 =  281 steps/epoch  â† Poucos updates, convergÃªncia lenta
```

**Impacto na taxa de aprendizado**: O learning rate Ã© aplicado ao gradiente acumulado (jÃ¡ normalizado). Com Transformers `Seq2SeqTrainer`, a loss jÃ¡ Ã© dividida pelo `grad_accum_steps`, entÃ£o a escala Ã© automaticamente ajustada.

**ImplementaÃ§Ã£o** (em `finetuning/trainer.py`):

```python
Seq2SeqTrainingArguments(
    per_device_train_batch_size=8,       # batch real na GPU
    gradient_accumulation_steps=2,       # acumula 2 batches
    # â†’ batch efetivo = 8 Ã— 2 = 16
)
```

---

#### `--lr 1e-5` â­

**O que Ã©**: **Learning Rate** â€” a taxa de aprendizado controla o tamanho do passo na atualizaÃ§Ã£o dos pesos do modelo. Ã‰ o hiperparÃ¢metro mais crÃ­tico do treinamento.

**ReferÃªncia**: Loshchilov, I. & Hutter, F. (2019). *Decoupled Weight Decay Regularization*. In ICLR 2019. https://arxiv.org/abs/1711.05101 (AdamW)

**ReferÃªncia**: Howard, J. & Ruder, S. (2018). *Universal Language Model Fine-tuning for Text Classification*. In Proceedings of ACL 2018, pp. 328â€“339. https://aclanthology.org/P18-1031/ (recomendaÃ§Ã£o de LR para fine-tuning)

**Regra de atualizaÃ§Ã£o (AdamW)**:

$$\theta_{t+1} = \theta_t - \eta \cdot \left(\frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} + \lambda \cdot \theta_t\right)$$

Onde:
- $\eta = 10^{-5}$ Ã© o learning rate
- $\hat{m}_t$ = mÃ©dia mÃ³vel dos gradientes (momentum)
- $\hat{v}_t$ = mÃ©dia mÃ³vel dos gradientesÂ² (adaptaÃ§Ã£o)
- $\lambda = 0.01$ = weight decay
- $\epsilon = 10^{-8}$ = estabilidade numÃ©rica

**Por que 1e-5 (e nÃ£o mais ou menos)?**

```
Para fine-tuning de modelos prÃ©-treinados, a literatura recomenda LRs pequenas:

  PrÃ©-treinamento (do zero):    1e-4  a 1e-3   (pesos aleatÃ³rios, grandes passos)
  Fine-tuning (adaptaÃ§Ã£o):      1e-5  a 5e-5   (pesos jÃ¡ bons, passos pequenos) â† 
  Ajuste mÃ­nimo (few-shot):     1e-6  a 5e-6   (alterar o mÃ­nimo possÃ­vel)

  LR = 1e-3 (muito alto para fine-tuning):
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   â•±â•²  â•±â•²  â•±â•²                       â”‚  OscilaÃ§Ã£o destrutiva
    â”‚  â•±  â•²â•±  â•²â•±  â•²   â†’ Loss diverge     â”‚  Esquece conhecimento prÃ©-treinado
    â”‚ â•±              â•²                    â”‚  "Catastrophic forgetting"
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  LR = 1e-5 (ideal para fine-tuning):
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ â•²                                   â”‚  ConvergÃªncia suave
    â”‚  â•²                                  â”‚  Preserva conhecimento base
    â”‚   â•²___________________________      â”‚  Adapta ao domÃ­nio SciELO
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  LR = 1e-7 (muito baixo):
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”‚  ConvergÃªncia desprezÃ­vel
    â”‚                                     â”‚  Modelo quase nÃ£o muda
    â”‚                                     â”‚  DesperdÃ­cio de computaÃ§Ã£o
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Schedule linear com warmup** (implementado via `Seq2SeqTrainer`):

O LR nÃ£o Ã© constante â€” segue um schedule com warmup linear (500 steps) + decay linear atÃ© 0:

```
LR
1e-5 â”¤          â•±â•²
     â”‚         â•±  â•²
     â”‚        â•±    â•²
     â”‚       â•±      â•²
     â”‚      â•±        â•²
     â”‚     â•±          â•²
     â”‚    â•±            â•²
     â”‚   â•±              â•²
     â”‚  â•±                â•²
     â”‚ â•±                  â•²
0    â”¤â•±                    â•²_
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â†’ Steps
            500          13500

 Fase 1: WARMUP (steps 0â†’500)
   LR sobe linearmente de 0 atÃ© 1e-5
   â†’ Evita instabilidade no inÃ­cio (gradientes grandes com pesos nÃ£o calibrados)
   â†’ "Aquece" o otimizador: momentum (mÌ‚) e variÃ¢ncia (vÌ‚) do AdamW estabilizam

 Fase 2: DECAY LINEAR (steps 500â†’13500)
   LR decresce linearmente de 1e-5 atÃ© ~0
   â†’ No inÃ­cio: passos maiores para aprender rÃ¡pido
   â†’ No final: passos minÃºsculos para refinamento fino

 Valores reais observados no treinamento:
   Step   100: lr = 1.98e-06  (warmup: subindo)
   Step   500: lr = 9.98e-06  (pico: ~1e-5)
   Step  1000: lr = 9.62e-06  (inÃ­cio do decay)
   Step  5000: lr = 6.54e-06  (metade do treinamento)
   Step 10000: lr = 2.70e-06  (75% do treinamento)
   Step 13000: lr = 3.88e-07  (quase zero)
   Step 13500: lr = 3.08e-09  (final: praticamente zero)
```

**ReferÃªncia para warmup**: Goyal, P. et al. (2017). *Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour*. arXiv:1706.02677. https://arxiv.org/abs/1706.02677

**Por que warmup de 500 steps?**

```
Sem warmup:
  Step 0: Gradiente grande (loss alta) Ã— LR mÃ¡ximo â†’ passo enorme
  â†’ Pode "destruir" features prÃ©-treinadas nas primeiras iteraÃ§Ãµes
  â†’ FenÃ´meno: "loss spike" ou divergÃªncia precoce

Com warmup de 500 steps:
  Step 0:   LR â‰ˆ 0       â†’ passo quase nulo, gradientes estabilizam
  Step 250: LR â‰ˆ 5e-6    â†’ passos moderados, momentum calibrado
  Step 500: LR = 1e-5     â†’ passo mÃ¡ximo, otimizador calibrado âœ…
  
  500 steps = ~3% do treinamento total (13.500 steps)
  â†’ PrÃ¡tica padrÃ£o: warmup de 1-5% do total de steps
```

**Weight Decay ($\lambda = 0.01$) â€” RegularizaÃ§Ã£o L2 Desacoplada**

No AdamW (diferente do Adam clÃ¡ssico), o weight decay Ã© aplicado **diretamente aos pesos** em vez de ser adicionado ao gradiente. Isso Ã© chamado de "decoupled weight decay" (Loshchilov & Hutter, 2019):

$$\theta_{t+1} = (1 - \eta \cdot \lambda) \cdot \theta_t - \eta \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$$

O termo $(1 - \eta \cdot \lambda) = (1 - 10^{-5} \times 0.01) = 0.9999999$ encolhe levemente os pesos a cada step, penalizando pesos com magnitude alta. Isso previne que o modelo "memorize" padrÃµes com pesos extremos.

```
ComparaÃ§Ã£o: Adam clÃ¡ssico vs AdamW

Adam (L2 regularizado):                    AdamW (weight decay desacoplado):
  g' = g + Î»Â·Î¸   (adiciona ao gradiente)    Î¸' = Î¸ - Î·Â·Î»Â·Î¸  (encolhe direto)
  m = Î²â‚Â·m + (1-Î²â‚)Â·g'                     m = Î²â‚Â·m + (1-Î²â‚)Â·g
  v = Î²â‚‚Â·v + (1-Î²â‚‚)Â·g'Â²                    v = Î²â‚‚Â·v + (1-Î²â‚‚)Â·gÂ²
  Î¸ = Î¸ - Î· Â· mÌ‚/âˆšvÌ‚                         Î¸ = Î¸' - Î· Â· mÌ‚/âˆšvÌ‚

  Problema: Î» interage com Adam de          Correto: Î» aplicado independente
  forma nÃ£o-intuitiva â†’ escala do           do gradiente adaptativo â†’ efeito
  weight decay depende do LR adaptativo     constante e previsÃ­vel âœ…
```

**AdamW â€” Algoritmo Completo (Kingma & Ba, 2014; Loshchilov & Hutter, 2019)**:

$$m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t \quad \text{(1Âº momento â€” momentum)}$$
$$v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2 \quad \text{(2Âº momento â€” variÃ¢ncia)}$$
$$\hat{m}_t = \frac{m_t}{1 - \beta_1^t} \quad \text{(correÃ§Ã£o de viÃ©s do momentum)}$$
$$\hat{v}_t = \frac{v_t}{1 - \beta_2^t} \quad \text{(correÃ§Ã£o de viÃ©s da variÃ¢ncia)}$$
$$\theta_{t+1} = \theta_t - \eta \cdot \left(\frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} + \lambda \cdot \theta_t\right)$$

Com os valores deste projeto: $\beta_1 = 0.9$, $\beta_2 = 0.999$, $\epsilon = 10^{-8}$, $\lambda = 0.01$, $\eta = 10^{-5}$ (com schedule).

**ReferÃªncias**:
- Kingma, D. P. & Ba, J. (2014). *Adam: A Method for Stochastic Optimization*. In ICLR 2015. https://arxiv.org/abs/1412.6980
- Loshchilov, I. & Hutter, F. (2019). *Decoupled Weight Decay Regularization*. In ICLR 2019. https://arxiv.org/abs/1711.05101

---

#### `--fp16` (Mixed Precision Training)

**O que Ã©**: Treina com **precisÃ£o mista** â€” forward pass em FP16 (16 bits), backward pass e atualizaÃ§Ã£o de pesos em FP32 (32 bits).

**ReferÃªncia**: Micikevicius, P. et al. (2018). *Mixed Precision Training*. In ICLR 2018. https://arxiv.org/abs/1710.03740

**Por que usar**:

```
FP32 (32-bit float):  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â†’ 4 bytes por peso
FP16 (16-bit float):  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  â†’ 2 bytes por peso

                     FP32          FP16 (mixed)     Economia
Pesos do modelo:     880 MB        440 MB           50%
AtivaÃ§Ãµes:           768 MB        384 MB           50%
Gradientes:          880 MB        880 MB (FP32)      0% (mantido em FP32)
Estados Adam:       1760 MB       1760 MB (FP32)      0% (mantido em FP32)
                    â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total aprox:        4288 MB       3464 MB           ~19% menor âœ…
```

**Como funciona (Automatic Mixed Precision)**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Pesos copiados FP32 â†’ FP16 (master copy)  â”‚
â”‚ 2. Forward pass em FP16 (rÃ¡pido nos Tensors)  â”‚
â”‚    â†’ Loss calculada em FP16                   â”‚
â”‚ 3. Loss scaling (multiplica loss Ã— 65536)     â”‚
â”‚    â†’ Evita underflow de gradientes em FP16    â”‚
â”‚ 4. Backward pass: gradientes em FP16          â”‚
â”‚ 5. Gradientes â†’ FP32, divididos pelo scaler   â”‚
â”‚ 6. Optimizer.step() em FP32 (pesos master)    â”‚
â”‚ 7. Pesos FP32 â†’ FP16 para prÃ³ximo forward     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**BenefÃ­cios na RTX 4050**:

```
RTX 4050 possui Tensor Cores com suporte a FP16:
  - OperaÃ§Ãµes FP16: ~16.6 TFLOPS
  - OperaÃ§Ãµes FP32: ~8.3 TFLOPS
  â†’ FP16 Ã© ~2x mais rÃ¡pido para matmul/convolutions

Tempo estimado por epoch:
  FP32: ~25 min/epoch Ã— 12 = ~5.0 horas
  FP16: ~15 min/epoch Ã— 12 = ~3.0 horas  â† ~40% mais rÃ¡pido
```

---

#### `--max_seq_len 256` â­

**O que Ã©**: Comprimento mÃ¡ximo em tokens de cada sequÃªncia (source e target). SequÃªncias mais longas sÃ£o **truncadas**, mais curtas recebem **padding**.

**Por que 256 (e nÃ£o o padrÃ£o 128 ou o mÃ¡ximo 512)?**

O modelo T5 suporta atÃ© `n_positions=512` tokens. A escolha de 256 Ã© um compromisso entre capturar abstracts completos e usar VRAM de forma eficiente.

```
DistribuiÃ§Ã£o de comprimento dos abstracts SciELO (em tokens):

  Tokens â”‚
    512+ â”‚ â–                                    1.2% truncados com max=512
    480  â”‚ â–                                   
    448  â”‚ â–
    416  â”‚ â–
    384  â”‚ â–Œ
    352  â”‚ â–‹
    320  â”‚ â–ˆ
    288  â”‚ â–ˆâ–ˆ
    256  â”‚ â–ˆâ–ˆâ–ˆâ–                                 ~5% truncados com max=256
    224  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–Œ
    192  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
    160  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
    128  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          ~25% truncados com max=128 âŒ
     96  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
     64  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
     32  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
      0  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ NÂº de exemplos

â†’ max_seq_len=128 (padrÃ£o): trunca ~25% dos abstracts (perde informaÃ§Ã£o)
â†’ max_seq_len=256 (escolhido): trunca ~5% (bom compromisso) âœ…
â†’ max_seq_len=512 (mÃ¡ximo): trunca <2% mas usa 4x mais memÃ³ria
```

**Impacto na VRAM** â€” a memÃ³ria escala **quadraticamente** com o comprimento da sequÃªncia (self-attention):

$$\text{MemÃ³ria}_{attention} \propto \text{batch\_size} \times \text{num\_heads} \times \text{seq\_len}^2$$

```
MemÃ³ria de atenÃ§Ã£o por camada (batch=8, heads=12):

  max_seq_len=128:  8 Ã— 12 Ã— 128Â² Ã— 2 bytes  =  3.0 MB/layer  Ã— 24 layers = 72 MB
  max_seq_len=256:  8 Ã— 12 Ã— 256Â² Ã— 2 bytes  = 12.0 MB/layer  Ã— 24 layers = 288 MB  â† Nosso
  max_seq_len=512:  8 Ã— 12 Ã— 512Â² Ã— 2 bytes  = 48.0 MB/layer  Ã— 24 layers = 1152 MB

  128 â†’ 256: +216 MB (cabe na RTX 4050 âœ…)
  256 â†’ 512: +864 MB (risco de OOM com batch=8 âŒ)
```

**Efeito no truncamento**:

```
Abstract original (310 tokens):
  "The present study aimed to evaluate the effect of different
   concentrations of sodium hypochlorite on the bond strength
   of fiber posts cemented with self-adhesive resin cement to
   root dentin. Forty single-rooted bovine teeth were selected
   and decoronated. The root canals were prepared using [...more...]
   The results suggest that sodium hypochlorite concentration
   significantly affects the bond strength values."

Com max_seq_len=128 (truncado em â†“):
  "The present study aimed to evaluate the effect of different
   concentrations of sodium hypochlorite on the bond strength
   of fiber posts cemented with self-adhesive resin cement to
   root dentin. Forty single-rooted bovine teeth were..."
  â†’ PERDE a conclusÃ£o do abstract (informaÃ§Ã£o crÃ­tica!)

Com max_seq_len=256 (truncado em â†“):
  "The present study aimed to evaluate the effect of different
   concentrations of sodium hypochlorite on the bond strength
   of fiber posts cemented with self-adhesive resin cement to
   root dentin. Forty single-rooted bovine teeth were selected
   and decoronated. The root canals were prepared using [...]
   The results suggest that sodium hypochlorite concentration
   significantly affects the bond strength values."
  â†’ Captura introduÃ§Ã£o, mÃ©todo E conclusÃ£o âœ…
```

**ImplementaÃ§Ã£o** (em `finetuning/trainer.py`):

```python
def preprocess_function(examples):
    inputs = tokenizer(
        examples["abstract_en"],
        max_length=max_seq_len,    # â† 256
        truncation=True,           # Corta sequÃªncias maiores
        padding="max_length",      # Pad atÃ© max_seq_len
    )
    targets = tokenizer(
        text_target=examples["abstract_pt"],
        max_length=max_seq_len,    # â† 256
        truncation=True,
        padding="max_length",
    )
    inputs["labels"] = targets["input_ids"]
    # Mascarar PAD tokens com -100 (ignorados na loss)
    inputs["labels"] = [
        [(l if l != tokenizer.pad_token_id else -100) for l in label]
        for label in inputs["labels"]
    ]
    return inputs
```

---

#### `--early_stopping_patience 2`

**O que Ã©**: Para o treinamento se a `eval_loss` **nÃ£o melhorar** por 2 epochs consecutivos. Evita overfitting e desperdÃ­cio de computaÃ§Ã£o.

**ReferÃªncia**: Prechelt, L. (1998). *Early Stopping â€” But When?*. In Neural Networks: Tricks of the Trade, Lecture Notes in Computer Science, vol 1524, pp. 55â€“69. https://doi.org/10.1007/3-540-49430-8_3

**Como funciona** (implementado via `EarlyStoppingCallback` do Transformers):

```
                  patience = 2
                  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Epoch  eval_loss   Melhor?   Contador   AÃ§Ã£o
  â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    1    1.006836    Sim âœ…    0          Salva como melhor
    2    0.993096    Sim âœ…    0          Salva como melhor
    3    0.986074    Sim âœ…    0          Salva como melhor
    ...     ...        ...      ...        ...
   12    0.972978    Sim âœ…    0          Salva como melhor â­

  â†’ No nosso caso, eval_loss melhorou em TODAS as 12 epochs.
  â†’ Early stopping NUNCA foi acionado.
  â†’ Se tivÃ©ssemos configurado epochs=50, pararia
     quando 2 epochs consecutivos nÃ£o melhorassem.

  CenÃ¡rio hipotÃ©tico (se tivÃ©ssemos treinado mais):
  Epoch  eval_loss   Melhor?   Contador   AÃ§Ã£o
  â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   12    0.972978    Sim âœ…    0          Salva como melhor
   13    0.973100    NÃ£o âŒ    1          Esperando... (1/2)
   14    0.973200    NÃ£o âŒ    2          PARA âœ‹ (patience atingido)
   â†’ Carrega checkpoint do epoch 12 (melhor modelo)
```

**Por que patience=2 (e nÃ£o 1 ou 5)?**

```
patience=1: Muito agressivo â€” para no primeiro "tropeÃ§o"
  â†’ Pode parar prematuramente se houver flutuaÃ§Ã£o normal

patience=2: Equilibrado â€” permite 1 flutuaÃ§Ã£o mas evita desperdÃ­cio
  â†’ PrÃ¡tica padrÃ£o na literatura de NLP  âœ…

patience=5: Conservador â€” treina mais mesmo sem melhoria
  â†’ DesperdiÃ§a horas de GPU se o modelo jÃ¡ convergiu
```

---

#### `--skip_prepare`

**O que Ã©**: Pula a etapa de preparaÃ§Ã£o dos CSVs de treino/validaÃ§Ã£o/teste (jÃ¡ preparados anteriormente no STAGE 3). Sem este flag, o script executaria `data_utils.prepare_evaluation_csv()` novamente.

**Quando usar**: Quando os arquivos `scielo_abstracts_train.csv`, `scielo_abstracts_val.csv` e `scielo_abstracts_test.csv` jÃ¡ existem no diretÃ³rio `finetuning/abstracts-datasets/`.

---

#### Gradient Checkpointing (ativado automaticamente no cÃ³digo)

**O que Ã©**: TÃ©cnica que **recalcula** ativaÃ§Ãµes intermediÃ¡rias durante o backward pass em vez de armazenÃ¡-las na memÃ³ria. Troca computaÃ§Ã£o por memÃ³ria.

**ReferÃªncia**: Chen, T. et al. (2016). *Training Deep Nets with Sublinear Memory Cost*. arXiv:1604.06174. https://arxiv.org/abs/1604.06174

```
SEM gradient checkpointing:
  Forward:  layerâ‚ â†’ [salva aâ‚] â†’ layerâ‚‚ â†’ [salva aâ‚‚] â†’ ... â†’ layerâ‚‚â‚„ â†’ [salva aâ‚‚â‚„] â†’ loss
  Backward: usa aâ‚‚â‚„ â†’ âˆ‡â‚‚â‚„, usa aâ‚‚â‚ƒ â†’ âˆ‡â‚‚â‚ƒ, ..., usa aâ‚ â†’ âˆ‡â‚

  MemÃ³ria: O(n) ativaÃ§Ãµes armazenadas = 24 camadas Ã— ativaÃ§Ãµes
  â†’ Pode exigir >6 GB (impossÃ­vel na RTX 4050)

COM gradient checkpointing:
  Forward:  layerâ‚ â†’ [salva aâ‚] â†’ layerâ‚‚ â†’ [descarta] â†’ ... â†’ layerâ‚‚â‚„ â†’ loss
  Backward: recalcula aâ‚‚â‚ƒ (forward parcial) â†’ âˆ‡â‚‚â‚ƒ, recalcula aâ‚‚â‚‚ â†’ âˆ‡â‚‚â‚‚, ...

  MemÃ³ria: O(âˆšn) ativaÃ§Ãµes armazenadas â‰ˆ âˆš24 â‰ˆ 5 checkpoints
  â†’ Economia de ~60-70% de VRAM das ativaÃ§Ãµes
  â†’ Custo: ~33% mais lento (recalcula forward para cada segmento)
```

**ImplementaÃ§Ã£o** (em `finetuning/trainer.py`):

```python
if hasattr(model, "gradient_checkpointing_enable"):
    model.gradient_checkpointing_enable()
    # â†’ Reduz VRAM de ativaÃ§Ãµes de ~1.5 GB para ~500 MB
    # â†’ Permite batch_size=8 com max_seq_len=256 na RTX 4050
```

---

### Resumo: Por que cada parÃ¢metro foi escolhido

| ParÃ¢metro        | Valor   | MotivaÃ§Ã£o principal                                         | Alternativa descartada        |
|------------------|---------|-------------------------------------------------------------|-------------------------------|
| `model`          | unicamp-t5 | Melhor trade-off qualidade/tamanho (220M params)         | Helsinki (600M, nÃ£o cabe)     |
| `epochs`         | 12      | eval_loss convergiu sem overfitting                         | 5 (underfitting), 50 (desnecessÃ¡rio) |
| `batch_size`     | 8       | Maior batch que cabe na RTX 4050 (6GB) com margem          | 16 (risco OOM), 4 (muito ruidoso) |
| `grad_accum`     | 2       | Batch efetivo=16, equilÃ­brio ruÃ­do/estabilidade             | 1 (ruidoso), 4 (lento demais) |
| `lr`             | 1e-5    | LR recomendado para fine-tuning de Transformers             | 1e-3 (catastrophic forgetting), 1e-7 (sem aprendizado) |
| `fp16`           | True    | ~40% mais rÃ¡pido + ~19% menos VRAM nos Tensor Cores        | FP32 (mais lento, mais VRAM)  |
| `max_seq_len`    | 256     | Captura ~95% dos abstracts sem OOM                          | 128 (perde 25%), 512 (OOM)    |
| `early_stopping` | 2       | Previne overfitting sem parar prematuramente                | 1 (agressivo demais), 5 (desperdiÃ§a GPU) |
| `skip_prepare`   | True    | Dados jÃ¡ preparados no STAGE 3                              | False (refaz splits desnecessariamente) |

### Curva de ConvergÃªncia (eval_loss)

```
Epoch | eval_loss | Step   | TendÃªncia
------|-----------|--------|----------
  1   | 1.006836  |  1125  |
  2   | 0.993096  |  2250  | â†“ melhorou
  3   | 0.986074  |  3375  | â†“ melhorou
  4   | 0.981832  |  4500  | â†“ melhorou
  5   | 0.979202  |  5625  | â†“ melhorou
  6   | 0.977226  |  6750  | â†“ melhorou
  7   | 0.975687  |  7875  | â†“ melhorou
  8   | 0.974656  |  9000  | â†“ melhorou
  9   | 0.973745  | 10125  | â†“ melhorou
 10   | 0.973330  | 11250  | â†“ melhorou
 11   | 0.973035  | 12375  | â†“ melhorou
 12   | 0.972978  | 13500  | â†“ melhorou â­ BEST
```

**ObservaÃ§Ãµes:**
- A eval_loss melhorou consistentemente em todas as 12 epochs
- O melhor checkpoint foi o Ãºltimo: `checkpoint-13500` (epoch 12, eval_loss: 0.972978)
- Early stopping NÃƒO foi acionado â€” o modelo ainda estava convergindo
- A taxa de melhoria desacelera nos epochs finais (~0.0003 por epoch), sugerindo proximidade do ponto Ã³timo

### Training Loss (mÃ©dia por epoch)

| Epoch | Training Loss (mÃ©dia) | Eval Loss  | Learning Rate (final) |
|-------|----------------------:|-----------:|----------------------:|
| 1     | 1.1014                | 1.006836   | 9.54e-06              |
| 2     | 1.0509                | 0.993096   | 8.69e-06              |
| 3     | 1.0334                | 0.986074   | 7.85e-06              |
| 4     | 1.0171                | 0.981832   | 6.92e-06              |
| 5     | 1.0028                | 0.979202   | 6.08e-06              |
| 6     | 0.9968                | 0.977226   | 5.23e-06              |
| 7     | 0.9839                | 0.975687   | 4.39e-06              |
| 8     | 0.9800                | 0.974656   | 3.46e-06              |
| 9     | 0.9748                | 0.973745   | 2.62e-06              |
| 10    | 0.9729                | 0.973330   | 1.77e-06              |
| 11    | 0.9664                | 0.973035   | 9.26e-07              |
| 12    | 0.9663                | 0.972978   | 3.08e-09              |

**ObservaÃ§Ãµes sobre o treinamento:**
- Training loss caiu de ~1.10 (epoch 1) para ~0.97 (epoch 12) â€” reduÃ§Ã£o de ~12%
- Learning rate seguiu schedule linear com warmup de 500 steps (pico 1e-5) e decay atÃ© ~0
- Gradient norms estÃ¡veis em 0.5â€“0.9 ao longo de todo o treinamento (sem gradient explosion)
- DiferenÃ§a train_loss vs eval_loss pequena (~0.01), indicando ausÃªncia de overfitting

### Detalhes TÃ©cnicos do Treinamento

- **Gradient checkpointing**: Reduz consumo de VRAM recalculando ativaÃ§Ãµes intermediÃ¡rias no backward pass
- **FP16 (mixed precision)**: Reduz uso de memÃ³ria e acelera computaÃ§Ã£o em Tensor Cores
- **Mascaramento de PAD tokens**: Labels com token PAD sÃ£o substituÃ­dos por -100 para nÃ£o contribuÃ­rem na cross-entropy loss
- **Early stopping**: Monitora `eval_loss` a cada epoch; para se nÃ£o houver melhoria em 2 epochs consecutivos
- **AdamW**: Otimizador Adam com weight decay desacoplado (0.01)

### Checkpoints

Cada epoch gera um checkpoint. Os 2 Ãºltimos sÃ£o preservados (save_total_limit=2):

| Checkpoint       | Epoch | eval_loss |
|------------------|-------|-----------|
| checkpoint-12375 | 11    | 0.973035  |
| checkpoint-13500 | 12    | 0.972978 â­ |

O modelo final (melhor) Ã© salvo na raiz: `unicamp-t5/unicamp-t5/`

### Resumir Treinamento Interrompido

```bash
python finetuning/finetune_selected_models.py \
  --model unicamp-t5 \
  --epochs 12 \
  --batch_size 8 \
  --grad_accum_steps 2 \
  --lr 1e-5 \
  --fp16 \
  --max_seq_len 256 \
  --early_stopping_patience 2 \
  --skip_prepare \
  --resume_from ./unicamp-t5/unicamp-t5/checkpoint-13500
```

O `Seq2SeqTrainer` preserva: estado do otimizador/scheduler, epoch/step atual, melhor modelo e contador de early stopping.

---

## STAGE 5: AvaliaÃ§Ã£o Final

### Objetivo
Comparar o modelo **antes** e **depois** do fine-tuning, usando os **mesmos** 5.000 exemplos de teste.

### Comandos

```bash
# Testar modelo base (antes do fine-tuning)
python finetuning/select_and_test_models.py --model unicamp-t5 --skip_prepare

# Testar modelo fine-tuned
python finetuning/select_and_test_models.py --test_finetuned --model unicamp-t5 --skip_prepare

# Testar ambos e comparar
python finetuning/select_and_test_models.py --test_both --model unicamp-t5 --skip_prepare
```

### Resultados

**Antes do fine-tuning** (`scielo_before_finetuning.csv`):

| Modelo     | BLEU  | chrF  | COMET  | BERTScore |
|------------|------:|------:|-------:|----------:|
| unicamp-t5 | 40.06 | 65.61 | 0.8499 | 0.8957    |

**ApÃ³s fine-tuning â€” Epoch 11** (`scielo_after_finetuning_epoch_11.csv`):

| Modelo     | Checkpoint       | BLEU  | chrF  | COMET  | BERTScore |
|------------|------------------|------:|------:|-------:|----------:|
| unicamp-t5 | checkpoint-12375 | 45.51 | 70.54 | 0.8756 | 0.9124    |

**ApÃ³s fine-tuning â€” Epoch 12** (`scielo_after_finetuning_epoch_12.csv`):

| Modelo     | Checkpoint       | BLEU  | chrF  | COMET  | BERTScore |
|------------|------------------|------:|------:|-------:|----------:|
| unicamp-t5 | checkpoint-13500 | 45.51 | 70.54 | 0.8756 | 0.9124    |

### AnÃ¡lise de Melhoria

| MÃ©trica    | Antes  | Depois (Ep.12) | Delta   | Melhoria |
|------------|-------:|---------------:|--------:|---------:|
| BLEU       | 40.06  | 45.51          | +5.45   | +13.6%   |
| chrF       | 65.61  | 70.54          | +4.93   | +7.5%    |
| COMET      | 0.8499 | 0.8756         | +0.0257 | +3.0%    |
| BERTScore  | 0.8957 | 0.9124         | +0.0167 | +1.9%    |

### InterpretaÃ§Ã£o

- **BLEU +13.6%**: Melhoria significativa na precisÃ£o de n-gramas. O modelo gera traduÃ§Ãµes com sobreposiÃ§Ã£o lexical mais prÃ³xima das referÃªncias humanas.
- **chrF +7.5%**: Melhoria a nÃ­vel de caracteres, indicando melhor morfologia e ortografia (acentuaÃ§Ã£o, concordÃ¢ncia).
- **COMET +3.0%**: Score neural baseado em modelo treinado em avaliaÃ§Ãµes humanas confirma melhoria na qualidade percebida.
- **BERTScore +1.9%**: Melhoria na similaridade semÃ¢ntica. O modelo preserva melhor o significado original.
- **Epochs 11â†’12 estÃ¡veis**: MÃ©tricas idÃªnticas entre epochs 11 e 12 indicam convergÃªncia atingida â€” o modelo estabilizou.
- **Melhoria dentro da faixa saudÃ¡vel** (+5 a +15% BLEU): Sem sinais de overfitting.

---

## Quickstart

```bash
# 1. Instalar dependÃªncias
pip install -r requirements.txt
pip install -r requirements-ml.txt

# 2. Preparar dataset SciELO
python prepare_scielo_dataset.py

# 3. Pipeline completo
python finetune_and_evaluate.py --skip_prepare

# Ou executar etapas individualmente:

# 3a. Preparar splits e testar modelo base
python finetuning/select_and_test_models.py --model unicamp-t5

# 3b. Fine-tuning
python finetuning/finetune_selected_models.py \
  --model unicamp-t5 --epochs 12 --batch_size 8 \
  --grad_accum_steps 2 --lr 1e-5 --fp16 --max_seq_len 256 \
  --early_stopping_patience 2 --skip_prepare

# 3c. Avaliar modelo fine-tuned
python finetuning/select_and_test_models.py --test_finetuned --model unicamp-t5 --skip_prepare
```

---

## DependÃªncias

### requirements.txt
DependÃªncias gerais do projeto (pandas, numpy, etc.)

### requirements-ml.txt
DependÃªncias de machine learning:
- `transformers` â€” HuggingFace Transformers (modelos, tokenizadores, Trainer)
- `torch` â€” PyTorch (backend de deep learning)
- `datasets` â€” HuggingFace Datasets
- `sacrebleu` â€” CÃ¡lculo de BLEU e chrF
- `unbabel-comet` â€” CÃ¡lculo de COMET
- `bert-score` â€” CÃ¡lculo de BERTScore
- `sentencepiece` â€” TokenizaÃ§Ã£o SentencePiece
- `accelerate` â€” AceleraÃ§Ã£o de treinamento HuggingFace
- `tqdm` â€” Barras de progresso

---

## Detalhes TÃ©cnicos

### Reprodutibilidade

- Seed = 42 em todos os splits e treinamento
- `torch.manual_seed(42)` no carregamento do modelo
- Splits determinÃ­sticos: mesmos 5k exemplos de teste para base e fine-tuned
- Resultados reprodutÃ­veis com mesma GPU e mesma seed

### Pipeline de TokenizaÃ§Ã£o e InferÃªncia

```
Entrada: "The patient presented with fever and cough."
    â†“ SentencePiece (unigram, 32k vocab)
Input IDs: [37, 1868, 4793, 28, 18851, 11, 14912, 5, 1]
    â†“ T5 Encoder (12 layers Ã— 768 dim Ã— 12 heads)
Hidden states: [768-dim vectors Ã— seq_len]
    â†“ T5 Decoder (12 layers, autoregressive, beam search k=5)
Output IDs: [101, 5847, 12059, 28, 18453, 11, 30419, 5, 1]
    â†“ Decode
SaÃ­da: "O paciente apresentou febre e tosse."
```

### CÃ¡lculo da Loss â€” Cross-Entropy

**ReferÃªncia**: Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press, Cap. 6.2.2. https://www.deeplearningbook.org/

A loss function utilizada Ã© a **Cross-Entropy** (entropia cruzada), que mede a diferenÃ§a entre a distribuiÃ§Ã£o de probabilidade prevista pelo modelo e a distribuiÃ§Ã£o real (one-hot do token correto).

$$\mathcal{L} = -\frac{1}{|T|} \sum_{t \in T} \log P(y_t \mid y_{<t}, X)$$

Onde:
- $y_t$ = token correto na posiÃ§Ã£o $t$ da traduÃ§Ã£o de referÃªncia
- $y_{<t}$ = todos os tokens anteriores (contexto autoregressivo do decoder)
- $X$ = sequÃªncia fonte completa (input do encoder)
- $P(y_t \mid y_{<t}, X)$ = probabilidade que o modelo atribui ao token correto
- $T$ = conjunto de tokens **nÃ£o-mascarados** (exclui tokens PAD)

**Como funciona na prÃ¡tica:**

```
ReferÃªncia: "O paciente apresentou febre" â†’ tokens [101, 5847, 12059, 18453, 1]
Decoder output (logits â†’ softmax â†’ probabilidades):

  PosiÃ§Ã£o 1: P("O")         = 0.87  â†’ -log(0.87) = 0.139
  PosiÃ§Ã£o 2: P("paciente")  = 0.72  â†’ -log(0.72) = 0.329
  PosiÃ§Ã£o 3: P("apresentou")= 0.65  â†’ -log(0.65) = 0.431
  PosiÃ§Ã£o 4: P("febre")     = 0.58  â†’ -log(0.58) = 0.545
  PosiÃ§Ã£o 5: P("</s>")      = 0.91  â†’ -log(0.91) = 0.094
  PosiÃ§Ã£o 6: [PAD] = -100           â†’ IGNORADO (nÃ£o contribui para loss)
  PosiÃ§Ã£o 7: [PAD] = -100           â†’ IGNORADO

  Loss = (0.139 + 0.329 + 0.431 + 0.545 + 0.094) / 5 = 0.308
```

**Mascaramento de PAD tokens**: Tokens de padding recebem label `-100`, que Ã© o valor especial do PyTorch `nn.CrossEntropyLoss(ignore_index=-100)`. Isso evita que o modelo aprenda a "gerar" padding â€” ele Ã© avaliado **apenas** pela qualidade dos tokens reais da traduÃ§Ã£o.

**RelaÃ§Ã£o com eval_loss**: A eval_loss reportada no treinamento (0.97 no epoch 12) Ã© exatamente esta cross-entropy calculada sobre os 2k exemplos de validaÃ§Ã£o. Um valor de 0.97 significa que, em mÃ©dia, o modelo atribui $e^{-0.97} \approx 0.38$ de probabilidade ao token correto â€” razoÃ¡vel para um vocabulÃ¡rio de 32k tokens (baseline aleatÃ³rio seria $-\log(1/32128) = 10.38$).

### Early Stopping

```
Para cada epoch:
  1. Calcular eval_loss no conjunto de validaÃ§Ã£o (2k exemplos)
  2. Se eval_loss < melhor_loss_anterior â†’ salvar como melhor modelo
  3. Se eval_loss >= melhor_loss_anterior â†’ incrementar contador
  4. Se contador >= patience (2) â†’ parar treinamento

No nosso caso: eval_loss melhorou em todas as 12 epochs,
portanto early stopping NÃƒO foi acionado.
```

### GeraÃ§Ã£o (InferÃªncia) â€” Beam Search

**ReferÃªncia**: Freitag, M. & Al-Onaizan, Y. (2017). *Beam Search Strategies for Neural Machine Translation*. In Proceedings of the First Workshop on Neural Machine Translation, pp. 56â€“60. https://aclanthology.org/W17-3207/

| ParÃ¢metro  | Valor          |
|------------|----------------|
| DecodificaÃ§Ã£o | Beam Search |
| Num beams  | 5              |
| Max length | 256 tokens     |

**O que Ã© Beam Search?** Em vez de escolher apenas o token mais provÃ¡vel a cada passo (greedy search), o Beam Search mantÃ©m as $k$ melhores hipÃ³teses parciais (beams) e expande todas:

$$\text{score}(y_{1:t}) = \sum_{i=1}^{t} \log P(y_i \mid y_{<i}, X)$$

```
Exemplo com num_beams=3 (simplificado):

Passo 1: gerar primeiro token
  Beam 1: "O"         score = log(0.87) = -0.139    âœ… Top-3
  Beam 2: "A"         score = log(0.05) = -2.996    âœ… Top-3
  Beam 3: "Os"        score = log(0.03) = -3.507    âœ… Top-3
  (outros 32125 tokens descartados)

Passo 2: expandir cada beam com prÃ³ximo token
  Beam 1 â†’ "O paciente"     score = -0.139 + log(0.72) = -0.468  âœ…
  Beam 1 â†’ "O doente"       score = -0.139 + log(0.10) = -2.442  âœ…
  Beam 2 â†’ "A paciente"     score = -2.996 + log(0.45) = -3.795  âœ…
  Beam 2 â†’ "A pessoa"       score = -2.996 + log(0.20) = -4.605
  Beam 3 â†’ "Os pacientes"   score = -3.507 + log(0.55) = -4.105
  ... (mantÃ©m apenas as 3 melhores hipÃ³teses)

Passo final: selecionar beam com maior score total
  Melhor: "O paciente apresentou febre persistente"  score = -3.21
  â†’ Esta Ã© a traduÃ§Ã£o retornada
```

**Por que `num_beams=5`?** Valores maiores exploram mais hipÃ³teses mas sÃ£o mais lentos ($O(k \times V \times T)$ onde $V$ = vocabulÃ¡rio, $T$ = comprimento). Para traduÃ§Ã£o, 4-5 beams Ã© o padrÃ£o na literatura (Vaswani et al., 2017).

---

## Estrutura do Projeto

```
.
â”œâ”€â”€ README.md                                  â† Este arquivo
â”œâ”€â”€ PROJECT_STRUCTURE.md                       â† Estrutura detalhada (visual)
â”œâ”€â”€ QUICK_COMMANDS.md                          â† ReferÃªncia rÃ¡pida de comandos
â”œâ”€â”€ requirements.txt                           â† DependÃªncias gerais
â”œâ”€â”€ requirements-ml.txt                        â† DependÃªncias ML
â”‚
â”œâ”€â”€ prepare_scielo_dataset.py                  [STAGE 0] Gera abstracts_scielo.csv
â”œâ”€â”€ models-test.py                             [STAGE 1] Avalia 5 modelos em datasets pÃºblicos
â”œâ”€â”€ evaluate_quickmt.py                        [STAGE 1] Avalia modelo QuickMT (CTranslate2)
â”œâ”€â”€ choose_best_model.py                       [STAGE 2] Ranking e seleÃ§Ã£o de modelo
â”œâ”€â”€ show_model_configs.py                      Exibe configuraÃ§Ãµes dos modelos
â”œâ”€â”€ compute_neural_metrics.py                  Calcula COMET e BERTScore
â”œâ”€â”€ finetune_and_evaluate.py                   Pipeline integrado (STAGES 1-5)
â”œâ”€â”€ check_gpu.py                               VerificaÃ§Ã£o de GPU disponÃ­vel
â”‚
â”œâ”€â”€ scielo_before_finetuning.csv               [STAGE 5] MÃ©tricas baseline (BLEU=40.06)
â”œâ”€â”€ scielo_after_finetuning_epoch_1.csv        [STAGE 5] MÃ©tricas epoch 1
â”œâ”€â”€ scielo_after_finetuning_epoch_11.csv       [STAGE 5] MÃ©tricas epoch 11 (BLEU=45.51)
â”œâ”€â”€ scielo_after_finetuning_epoch_12.csv       [STAGE 5] MÃ©tricas epoch 12 (BLEU=45.51)
â”‚
â”œâ”€â”€ evaluation/                                MÃ³dulo de avaliaÃ§Ã£o (STAGE 1)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                              ConfiguraÃ§Ãµes de avaliaÃ§Ã£o
â”‚   â”œâ”€â”€ datasets.py                            Carregamento de datasets pÃºblicos
â”‚   â”œâ”€â”€ metrics.py                             CÃ¡lculo de mÃ©tricas
â”‚   â”œâ”€â”€ models_loader.py                       Carregamento de modelos
â”‚   â”œâ”€â”€ run.py                                 ExecuÃ§Ã£o da avaliaÃ§Ã£o
â”‚   â”œâ”€â”€ io_utils.py                            UtilitÃ¡rios de I/O
â”‚   â””â”€â”€ fill_missing_metrics.py                Preenche mÃ©tricas faltantes
â”‚
â”œâ”€â”€ evaluation_results/                        Resultados de avaliaÃ§Ã£o
â”‚   â”œâ”€â”€ translation_metrics_all.csv            [STAGE 1] Consolidado todos os modelos
â”‚   â”œâ”€â”€ Helsinki-NLP_opus-mt-tc-big-en-pt.csv
â”‚   â”œâ”€â”€ Narrativa_mbart-large-50-finetuned-opus-en-pt-translation.csv
â”‚   â”œâ”€â”€ unicamp-dl_translation-en-pt-t5.csv
â”‚   â”œâ”€â”€ VanessaSchenkel_unicamp-finetuned-en-to-pt-dataset-ted.csv
â”‚   â”œâ”€â”€ danhsf_m2m100_418M-finetuned-kde4-en-to-pt_BR.csv
â”‚   â””â”€â”€ quickmt_quickmt-en-pt.csv
â”‚
â”œâ”€â”€ finetuning/                                MÃ³dulo de fine-tuning (STAGES 3-5)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                              ConfiguraÃ§Ãµes centralizadas
â”‚   â”œâ”€â”€ models.py                              Carregamento/salvamento de modelos
â”‚   â”œâ”€â”€ data_utils.py                          PreparaÃ§Ã£o de dados (splits)
â”‚   â”œâ”€â”€ datasets.py                            Dataset handling
â”‚   â”œâ”€â”€ metrics.py                             BLEU, chrF, COMET, BERTScore
â”‚   â”œâ”€â”€ evaluate.py                            AvaliaÃ§Ã£o com progresso (tqdm)
â”‚   â”œâ”€â”€ trainer.py                             Seq2SeqTrainer + fine-tuning loop
â”‚   â”œâ”€â”€ compare.py                             ComparaÃ§Ã£o base vs fine-tuned
â”‚   â”œâ”€â”€ io_utils.py                            UtilitÃ¡rios I/O
â”‚   â”œâ”€â”€ finetune_selected_models.py            [STAGE 4] Script de fine-tuning
â”‚   â”œâ”€â”€ select_and_test_models.py              [STAGE 3+5] Preparo + teste
â”‚   â””â”€â”€ abstracts-datasets/                    [STAGE 3] Dados SciELO
â”‚       â”œâ”€â”€ abstracts_scielo.csv               Corpus completo (2.7M exemplos)
â”‚       â”œâ”€â”€ scielo_abstracts_train.csv         18.000 exemplos (treino)
â”‚       â”œâ”€â”€ scielo_abstracts_val.csv            2.000 exemplos (validaÃ§Ã£o)
â”‚       â””â”€â”€ scielo_abstracts_test.csv           5.000 exemplos (teste)
â”‚
â”œâ”€â”€ unicamp-t5/                                â­ MODELO FINE-TUNED (resultado final)
â”‚   â””â”€â”€ unicamp-t5/
â”‚       â”œâ”€â”€ config.json                        ConfiguraÃ§Ã£o do modelo
â”‚       â”œâ”€â”€ generation_config.json             ConfiguraÃ§Ã£o de geraÃ§Ã£o
â”‚       â”œâ”€â”€ model.safetensors                  Pesos do melhor modelo (epoch 12)
â”‚       â”œâ”€â”€ tokenizer.json                     Tokenizador
â”‚       â”œâ”€â”€ tokenizer_config.json              ConfiguraÃ§Ã£o do tokenizador
â”‚       â”œâ”€â”€ spiece.model                       Modelo SentencePiece
â”‚       â”œâ”€â”€ special_tokens_map.json
â”‚       â”œâ”€â”€ checkpoint-12375/                  Checkpoint epoch 11
â”‚       â””â”€â”€ checkpoint-13500/                  Checkpoint epoch 12 (best)
â”‚           â”œâ”€â”€ model.safetensors
â”‚           â”œâ”€â”€ optimizer.pt
â”‚           â”œâ”€â”€ scheduler.pt
â”‚           â”œâ”€â”€ trainer_state.json             Log completo de treinamento
â”‚           â””â”€â”€ training_args.bin
â”‚
â”œâ”€â”€ models/                                    Modelos auxiliares
â”‚   â””â”€â”€ finetuned-scielo/
â”‚       â””â”€â”€ helsinki/                           Fine-tuning anterior (Helsinki)
â”‚
â”œâ”€â”€ models-configs/                            ConfiguraÃ§Ãµes JSON dos modelos
â”‚   â”œâ”€â”€ helsink.json
â”‚   â””â”€â”€ m2m100.json
â”‚
â””â”€â”€ checkpoints/                               Checkpoints de controle
    â”œâ”€â”€ training/
    â””â”€â”€ evaluation/
```

---

## MÃ©tricas de AvaliaÃ§Ã£o â€” ExplicaÃ§Ã£o TÃ©cnica

Este projeto utiliza 4 mÃ©tricas complementares para avaliar a qualidade das traduÃ§Ãµes. Duas sÃ£o mÃ©tricas **lexicais** (baseadas em sobreposiÃ§Ã£o de tokens) e duas sÃ£o mÃ©tricas **neurais** (baseadas em embeddings de modelos prÃ©-treinados). A combinaÃ§Ã£o garante uma avaliaÃ§Ã£o robusta que captura tanto a fidelidade lexical quanto a adequaÃ§Ã£o semÃ¢ntica.

### VisÃ£o Geral

| MÃ©trica     | Tipo    | Granularidade | Escala   | Requer Source? | ImplementaÃ§Ã£o        |
|-------------|---------|---------------|----------|----------------|----------------------|
| BLEU        | Lexical | Palavra       | 0â€“100    | NÃ£o            | `sacrebleu.BLEU()`   |
| chrF        | Lexical | Caractere     | 0â€“100    | NÃ£o            | `sacrebleu.CHRF()`   |
| COMET       | Neural  | SentenÃ§a      | 0â€“1      | Sim            | `Unbabel/wmt22-comet-da` |
| BERTScore   | Neural  | Token         | 0â€“1      | NÃ£o            | `bert-score` (lang=pt) |

---

### 1. BLEU (Bilingual Evaluation Understudy)

**ReferÃªncia**: Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). *BLEU: a Method for Automatic Evaluation of Machine Translation*. In Proceedings of the 40th Annual Meeting of the ACL, pp. 311â€“318. https://aclanthology.org/P02-1040/

**PadronizaÃ§Ã£o**: Post, M. (2018). *A Call for Clarity in Reporting BLEU Scores*. In Proceedings of the Third Conference on Machine Translation (WMT), pp. 186â€“191. https://aclanthology.org/W18-6319/

#### O que mede
BLEU mede a **precisÃ£o de n-gramas** entre a traduÃ§Ã£o candidata (hipÃ³tese) e a traduÃ§Ã£o de referÃªncia humana, penalizando traduÃ§Ãµes muito curtas via *brevity penalty*. Ã‰ a mÃ©trica mais utilizada na literatura de traduÃ§Ã£o automÃ¡tica.

#### FÃ³rmula

$$\text{BLEU} = \text{BP} \cdot \exp\left(\sum_{n=1}^{N} w_n \cdot \log p_n\right)$$

Onde:
- $p_n$ = precisÃ£o de n-gramas modificada (clipped precision)
- $w_n = \frac{1}{N}$ (peso uniforme, $N=4$ por padrÃ£o)
- $\text{BP} = \min\left(1, \; e^{1 - r/c}\right)$ = brevity penalty ($r$ = comprimento da referÃªncia, $c$ = comprimento da hipÃ³tese)

#### Exemplo Visual

```
ReferÃªncia: "O paciente apresentou febre e tosse persistente"
HipÃ³tese:   "O paciente apresentou febre e tosse"

Unigrams (1-gram):
  ReferÃªncia: {O, paciente, apresentou, febre, e, tosse, persistente}  â†’ 7 tokens
  HipÃ³tese:   {O, paciente, apresentou, febre, e, tosse}              â†’ 6 tokens
  Match:      {O, paciente, apresentou, febre, e, tosse}              â†’ 6 matches
  pâ‚ = 6/6 = 1.00 âœ…

Bigrams (2-gram):
  ReferÃªncia: {O paciente, paciente apresentou, apresentou febre, febre e, e tosse, tosse persistente}
  HipÃ³tese:   {O paciente, paciente apresentou, apresentou febre, febre e, e tosse}
  Match:      {O paciente, paciente apresentou, apresentou febre, febre e, e tosse}  â†’ 5/5
  pâ‚‚ = 5/5 = 1.00 âœ…

Trigrams (3-gram):
  ReferÃªncia: {O paciente apresentou, paciente apresentou febre, apresentou febre e, febre e tosse, e tosse persistente}
  HipÃ³tese:   {O paciente apresentou, paciente apresentou febre, apresentou febre e, febre e tosse}
  Match:      {O paciente apresentou, paciente apresentou febre, apresentou febre e, febre e tosse}  â†’ 4/4
  pâ‚ƒ = 4/4 = 1.00 âœ…

4-grams:
  ReferÃªncia: {O paciente apresentou febre, paciente apresentou febre e, apresentou febre e tosse, febre e tosse persistente}
  HipÃ³tese:   {O paciente apresentou febre, paciente apresentou febre e, apresentou febre e tosse}
  Match:      {O paciente apresentou febre, paciente apresentou febre e, apresentou febre e tosse}  â†’ 3/3
  pâ‚„ = 3/3 = 1.00 âœ…

Brevity Penalty:
  r = 7 (referÃªncia), c = 6 (hipÃ³tese) â†’ c < r
  BP = exp(1 - 7/6) = exp(-0.167) â‰ˆ 0.846

BLEU = BP Ã— exp(Â¼ Ã— (log(1.0) + log(1.0) + log(1.0) + log(1.0)))
     = 0.846 Ã— exp(0)
     = 0.846 Ã— 1.0
     = 84.6   â† Penalizado por ser mais curta que a referÃªncia
```

#### LimitaÃ§Ãµes

- **InsensÃ­vel a sinÃ´nimos**: "febre" vs "temperatura alta" = 0 match, apesar de semanticamente equivalentes
- **Independente da ordem global**: PermutaÃ§Ãµes de fragmentos podem gerar BLEU alto sem coerÃªncia
- **Brevity penalty assimÃ©trica**: Penaliza traduÃ§Ãµes curtas, mas nÃ£o as longas demais

#### ImplementaÃ§Ã£o neste projeto

```python
# finetuning/metrics.py
from sacrebleu import BLEU
bleu = BLEU(lowercase=False)
bleu_score = bleu.corpus_score(predictions, [references])  # corpus-level
# Retorna: score âˆˆ [0, 100]
```

> **Nota**: Utilizamos `sacreBLEU` (Post, 2018) que garante tokenizaÃ§Ã£o padronizada e reprodutibilidade. O score Ã© computado a nÃ­vel de corpus (nÃ£o mÃ©dia de sentenÃ§as).

---

### 2. chrF (Character n-gram F-score)

**ReferÃªncia**: PopoviÄ‡, M. (2015). *chrF: character n-gram F-score for automatic MT evaluation*. In Proceedings of the Tenth Workshop on Statistical Machine Translation (WMT), pp. 392â€“395. https://aclanthology.org/W15-3049/

#### O que mede
chrF mede a **sobreposiÃ§Ã£o de n-gramas de caracteres** entre hipÃ³tese e referÃªncia, utilizando o F-score (mÃ©dia harmÃ´nica de precisÃ£o e recall). Por operar a nÃ­vel de caractere, Ã© mais robusta a variaÃ§Ãµes morfolÃ³gicas do que o BLEU.

#### FÃ³rmula

$$\text{chrF}_\beta = (1 + \beta^2) \cdot \frac{\text{chrP} \cdot \text{chrR}}{\beta^2 \cdot \text{chrP} + \text{chrR}}$$

Onde:
- $\text{chrP}_n = \frac{|\text{n-gramas}_{\text{hyp}} \cap \text{n-gramas}_{\text{ref}}|}{|\text{n-gramas}_{\text{hyp}}|}$ (precisÃ£o de char n-grams)
- $\text{chrR}_n = \frac{|\text{n-gramas}_{\text{hyp}} \cap \text{n-gramas}_{\text{ref}}|}{|\text{n-gramas}_{\text{ref}}|}$ (recall de char n-grams)
- $\beta = 2$ por padrÃ£o (favorece recall)
- MÃ©dia sobre $n = 1, 2, \ldots, 6$ (character n-grams de ordem 1 a 6)

#### Exemplo Visual

```
ReferÃªncia: "apresentou"
HipÃ³tese:   "apresentaram"

Character 3-grams:
  ReferÃªncia: {apr, pre, res, ese, sen, ent, nto, tou}           â†’ 8 trigrams
  HipÃ³tese:   {apr, pre, res, ese, sen, ent, nta, tar, ara, ram} â†’ 10 trigrams
  InterseÃ§Ã£o: {apr, pre, res, ese, sen, ent}                     â†’  6 matches

  chrPâ‚ƒ = 6/10 = 0.60 (precisÃ£o: quantos trigrams da hipÃ³tese estÃ£o na referÃªncia)
  chrRâ‚ƒ = 6/8  = 0.75 (recall: quantos trigrams da referÃªncia foram cobertos)

  chrFâ‚ƒ (Î²=2) = (1 + 4) Ã— (0.60 Ã— 0.75) / (4 Ã— 0.60 + 0.75)
              = 5 Ã— 0.45 / 3.15
              = 0.714

â†’ Apesar de conjugaÃ§Ãµes diferentes ("apresentou" vs "apresentaram"),
  chrF captura a similaridade morfolÃ³gica (71.4%) enquanto BLEU
  word-level daria 0% match (palavras diferentes).
```

#### Vantagens sobre BLEU

```
Exemplo: traduÃ§Ã£o com variaÃ§Ã£o morfolÃ³gica

ReferÃªncia: "Os pacientes foram diagnosticados"
HipÃ³tese A: "O paciente foi diagnosticado"         â† traduÃ§Ã£o correta (singular)
HipÃ³tese B: "A mesa voou pelo hospital"             â† traduÃ§Ã£o incorreta

BLEU (word-level):
  HipÃ³tese A: matches = {diagnosticado~diagnosticados?} â†’ match parcial
  HipÃ³tese B: matches = {}                              â†’ 0 matches
  â†’ BLEU diferencia, mas penaliza A severamente por flexÃ£o

chrF (char-level):
  HipÃ³tese A: alta sobreposiÃ§Ã£o em "pacient-", "diagnosticad-", "for-/foi"
  HipÃ³tese B: baixÃ­ssima sobreposiÃ§Ã£o
  â†’ chrF captura melhor que A Ã© quase correta
```

#### ImplementaÃ§Ã£o neste projeto

```python
# finetuning/metrics.py
from sacrebleu import CHRF
chrf = CHRF(lowercase=False)
chrf_score = chrf.corpus_score(predictions, [references])  # corpus-level
# Retorna: score âˆˆ [0, 100]
```

---

### 3. COMET (Crosslingual Optimized Metric for Evaluation of Translation)

**ReferÃªncia**: Rei, R., de Souza, J. G. C., Alves, D., Zerva, C., Farinha, A. C., Glushkova, T., Lavie, A., Coheur, L., & Martins, A. F. T. (2022). *COMET-22: Unbabel-IST 2022 Submission for the Metrics Shared Task*. In Proceedings of the Seventh Conference on Machine Translation (WMT), pp. 578â€“585. https://aclanthology.org/2022.wmt-1.52/

**Modelo base**: Conneau, A. et al. (2020). *Unsupervised Cross-lingual Representation Learning at Scale*. In Proceedings of ACL 2020, pp. 8440â€“8451. https://aclanthology.org/2020.acl-main.747/ (XLM-RoBERTa)

#### O que mede
COMET Ã© uma mÃ©trica **neural aprendida** que utiliza um modelo XLM-RoBERTa fine-tuned em avaliaÃ§Ãµes humanas (Direct Assessments) de competiÃ§Ãµes WMT. Diferente de BLEU e chrF, COMET considera a **frase fonte** (source) alÃ©m da referÃªncia e hipÃ³tese, capturando **adequaÃ§Ã£o** (se o significado foi preservado) e **fluÃªncia**.

#### FÃ³rmula de treinamento

O modelo COMET Ã© treinado para minimizar o erro quadrÃ¡tico mÃ©dio (MSE) entre o score previsto e avaliaÃ§Ãµes humanas (Direct Assessments, DA):

$$\mathcal{L}_{COMET} = \frac{1}{N} \sum_{i=1}^{N} \left( f(\mathbf{e}_{src}^i, \mathbf{e}_{mt}^i, \mathbf{e}_{ref}^i) - z_i \right)^2$$

Onde:
- $f(\cdot)$ = rede feed-forward estimadora (output: score predito)
- $\mathbf{e}_{src}, \mathbf{e}_{mt}, \mathbf{e}_{ref}$ = embeddings pooled do XLM-R para source, hipÃ³tese e referÃªncia
- $z_i$ = z-score da avaliaÃ§Ã£o humana (Direct Assessment normalizado)
- $N$ = nÃºmero de exemplos de treinamento (avaliaÃ§Ãµes WMT15â€“WMT20)

A entrada do estimador combina os embeddings em um vetor de features:

$$\mathbf{f} = [\mathbf{e}_{src}; \, \mathbf{e}_{mt}; \, \mathbf{e}_{ref}; \, |\mathbf{e}_{src} - \mathbf{e}_{mt}|; \, |\mathbf{e}_{ref} - \mathbf{e}_{mt}|; \, \mathbf{e}_{src} \odot \mathbf{e}_{mt}; \, \mathbf{e}_{ref} \odot \mathbf{e}_{mt}]$$

Onde $[\,;\,]$ Ã© concatenaÃ§Ã£o, $|\cdot|$ Ã© diferenÃ§a absoluta, e $\odot$ Ã© produto elemento a elemento. Isso captura **similaridade**, **diferenÃ§a** e **interaÃ§Ã£o** entre os pares.

#### Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COMET-22                          â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  Source   â”‚  â”‚ HipÃ³tese â”‚  â”‚ReferÃªnciaâ”‚          â”‚
â”‚  â”‚  (EN)    â”‚  â”‚  (MT)    â”‚  â”‚  (REF)   â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â”‚
â”‚       â”‚              â”‚              â”‚                â”‚
â”‚       â–¼              â–¼              â–¼                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚         XLM-RoBERTa (encoder)           â”‚        â”‚
â”‚  â”‚      (550M params, 24 layers)           â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚       â”‚              â”‚              â”‚                â”‚
â”‚       â–¼              â–¼              â–¼                â”‚
â”‚   emb_src        emb_mt         emb_ref             â”‚
â”‚       â”‚              â”‚              â”‚                â”‚
â”‚       â–¼              â–¼              â–¼                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚     Pooling + Feature Extraction         â”‚        â”‚
â”‚  â”‚  [emb_src; emb_mt; emb_ref;             â”‚        â”‚
â”‚  â”‚   |emb_src - emb_mt|;                   â”‚        â”‚
â”‚  â”‚   |emb_ref - emb_mt|;                   â”‚        â”‚
â”‚  â”‚   emb_src * emb_mt;                     â”‚        â”‚
â”‚  â”‚   emb_ref * emb_mt]                     â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                   â”‚                                  â”‚
â”‚                   â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚       Estimator (Feed-Forward)           â”‚        â”‚
â”‚  â”‚       â†’ score âˆˆ [0, 1]                   â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Exemplo Visual

```
Source:    "The patient presented with persistent fever and dry cough."
ReferÃªncia: "O paciente apresentou febre persistente e tosse seca."
HipÃ³tese A: "O paciente apresentou febre persistente e tosse seca."   â†’ COMET â‰ˆ 1.00
HipÃ³tese B: "O paciente teve febre contÃ­nua e tosse sem catarro."     â†’ COMET â‰ˆ 0.88
HipÃ³tese C: "O doente mostrou uma febre que nÃ£o passa e tosse."       â†’ COMET â‰ˆ 0.80
HipÃ³tese D: "A mesa apresentou febre e tosse."                        â†’ COMET â‰ˆ 0.35

AnÃ¡lise:
  HipÃ³tese A: TraduÃ§Ã£o perfeita                    â†’ score mÃ¡ximo
  HipÃ³tese B: Semanticamente correta, lÃ©xico diferente
              XLM-R captura que "contÃ­nua" â‰ˆ "persistente"
              e "sem catarro" â‰ˆ "seca"              â†’ score alto
  HipÃ³tese C: Significado preservado, estilo informal
              "doente" â‰ˆ "paciente", "que nÃ£o passa" â‰ˆ "persistente"
              â†’ COMET detecta adequaÃ§Ã£o semÃ¢ntica     â†’ score bom
  HipÃ³tese D: Erro semÃ¢ntico grave ("mesa" â‰  "patient")
              COMET usa o source para detectar inconsistÃªncia
              â†’ score baixo

Nota: BLEU daria score ZERO para B e C (sem match exato de n-gramas),
      mas COMET reconhece que sÃ£o traduÃ§Ãµes vÃ¡lidas.
```

#### Por que COMET usa o source?

```
Source:     "The bank collapsed after the flood."
ReferÃªncia: "O banco desabou apÃ³s a enchente."

HipÃ³tese A: "O banco desabou apÃ³s a enchente."     â†’ COMET alto
HipÃ³tese B: "A instituiÃ§Ã£o bancÃ¡ria faliu."         â†’ COMET baixo

Sem o source, a HipÃ³tese B poderia parecer uma parÃ¡frase razoÃ¡vel.
Mas o source diz "flood" (enchente), nÃ£o "financial crisis".
COMET detecta que "banco" = margem do rio (nÃ£o instituiÃ§Ã£o financeira),
e que "collapsed" = desabou fisicamente (nÃ£o faliu).
â†’ O acesso ao source resolve ambiguidades e melhora a correlaÃ§Ã£o
  com julgamentos humanos.
```

#### ImplementaÃ§Ã£o neste projeto

```python
# finetuning/metrics.py
from comet import download_model, load_from_checkpoint

model_path = download_model("Unbabel/wmt22-comet-da")
comet_model = load_from_checkpoint(model_path)
comet_model.eval()

data = [
    {"src": src, "mt": pred, "ref": ref}
    for src, pred, ref in zip(sources, predictions, references)
]
output = comet_model.predict(data, batch_size=2, gpus=1)
system_score = float(output.system_score)  # mÃ©dia âˆˆ [0, 1]
```

> **Nota**: O modelo utilizado Ã© `Unbabel/wmt22-comet-da`, treinado em Direct Assessments (DA) de competiÃ§Ãµes WMT15â€“WMT20. Requer ~2GB de VRAM adicionais (XLM-R large). Por isso, o modelo de traduÃ§Ã£o Ã© movido para CPU antes do cÃ¡lculo do COMET.

---

### 4. BERTScore

**ReferÃªncia**: Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., & Artzi, Y. (2020). *BERTScore: Evaluating Text Generation with BERT*. In International Conference on Learning Representations (ICLR 2020). https://openreview.net/forum?id=SkeHuCVFDr

#### O que mede
BERTScore calcula a **similaridade semÃ¢ntica** entre hipÃ³tese e referÃªncia usando **embeddings contextuais** de um modelo BERT prÃ©-treinado. Em vez de comparar tokens exatos (como BLEU), compara representaÃ§Ãµes vetoriais que codificam o significado no contexto.

#### FÃ³rmula

Para cada token $x_i$ da referÃªncia e $\hat{x}_j$ da hipÃ³tese, calcula-se a similaridade por cosseno dos embeddings contextuais:

$$\text{Recall} = \frac{1}{|x|} \sum_{x_i \in x} \max_{\hat{x}_j \in \hat{x}} \; \mathbf{x}_i^\top \hat{\mathbf{x}}_j$$

$$\text{Precision} = \frac{1}{|\hat{x}|} \sum_{\hat{x}_j \in \hat{x}} \max_{x_i \in x} \; \mathbf{x}_i^\top \hat{\mathbf{x}}_j$$

$$F_1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$$

(Onde $\mathbf{x}_i$ e $\hat{\mathbf{x}}_j$ sÃ£o embeddings contextuais L2-normalizados)

#### Exemplo Visual

```
ReferÃªncia: "O paciente apresentou febre persistente"
HipÃ³tese:   "O doente teve temperatura alta contÃ­nua"

Passo 1: Gerar embeddings contextuais (BERT/mBERT)

  ReferÃªncia:  O         paciente   apresentou   febre       persistente
               [vâ‚]      [vâ‚‚]       [vâ‚ƒ]         [vâ‚„]        [vâ‚…]

  HipÃ³tese:    O         doente     teve         temperatura  alta       contÃ­nua
               [Ä¥â‚]      [Ä¥â‚‚]       [Ä¥â‚ƒ]         [Ä¥â‚„]        [Ä¥â‚…]       [Ä¥â‚†]

Passo 2: Calcular matriz de similaridade por cosseno

               Ä¥â‚(O)  Ä¥â‚‚(doente)  Ä¥â‚ƒ(teve)  Ä¥â‚„(temperatura)  Ä¥â‚…(alta)  Ä¥â‚†(contÃ­nua)
  vâ‚(O)        0.99    0.12        0.08       0.05              0.03       0.04
  vâ‚‚(paciente) 0.15    0.87 â†max   0.10       0.08              0.05       0.06
  vâ‚ƒ(apresentou)0.10   0.12        0.72 â†max  0.06              0.04       0.08
  vâ‚„(febre)    0.05    0.09        0.07       0.83 â†max         0.45       0.12
  vâ‚…(persistente)0.03  0.06        0.05       0.15              0.30       0.82 â†max

Passo 3: Greedy matching (cada token â†’ melhor match)

  Recall (para cada token da referÃªncia, max cosseno com hipÃ³tese):
    O           â†’ max(0.99, 0.12, 0.08, 0.05, 0.03, 0.04) = 0.99
    paciente    â†’ max(0.15, 0.87, 0.10, 0.08, 0.05, 0.06) = 0.87  â† "doente" capturado!
    apresentou  â†’ max(0.10, 0.12, 0.72, 0.06, 0.04, 0.08) = 0.72  â† "teve" capturado!
    febre       â†’ max(0.05, 0.09, 0.07, 0.83, 0.45, 0.12) = 0.83  â† "temperatura" capturado!
    persistente â†’ max(0.03, 0.06, 0.05, 0.15, 0.30, 0.82) = 0.82  â† "contÃ­nua" capturado!

  Recall = (0.99 + 0.87 + 0.72 + 0.83 + 0.82) / 5 = 0.846

  Precision (para cada token da hipÃ³tese, max cosseno com referÃªncia):
    O           â†’ 0.99
    doente      â†’ 0.87 (â† "paciente")
    teve        â†’ 0.72 (â† "apresentou")
    temperatura â†’ 0.83 (â† "febre")
    alta        â†’ 0.45 (â† "febre", match parcial)
    contÃ­nua    â†’ 0.82 (â† "persistente")

  Precision = (0.99 + 0.87 + 0.72 + 0.83 + 0.45 + 0.82) / 6 = 0.780

  Fâ‚ = 2 Ã— (0.780 Ã— 0.846) / (0.780 + 0.846) = 0.812

â†’ BERTScore Fâ‚ = 0.812 (alto!)
  Apesar de ZERO palavras idÃªnticas (exceto "O"),
  BERTScore reconhece equivalÃªncia semÃ¢ntica:
    paciente â†” doente          (sinÃ´nimos)
    apresentou â†” teve          (verbos relacionados)
    febre â†” temperatura alta   (conceito mÃ©dico equivalente)
    persistente â†” contÃ­nua     (sinÃ´nimos)
```

#### ImplementaÃ§Ã£o neste projeto

```python
# finetuning/metrics.py
from bert_score import score

P, R, F1 = score(
    predictions,       # list[str] â€” traduÃ§Ãµes do modelo
    references,        # list[str] â€” referÃªncias humanas
    lang="pt",         # seleciona modelo multilÃ­ngue adequado
    batch_size=2,      # batch pequeno para caber na GPU
    device="cuda"
)
bertscore_f1 = float(F1.mean())  # mÃ©dia âˆˆ [0, 1]
```

> **Nota**: O parÃ¢metro `lang="pt"` seleciona automaticamente o modelo BERT multilÃ­ngue adequado para portuguÃªs. A mÃ©trica Ã© computada por sentenÃ§a e depois promediada a nÃ­vel de corpus. O modelo BERT Ã© carregado apÃ³s liberar o modelo de traduÃ§Ã£o da GPU para evitar OOM na RTX 4050 (6GB VRAM).

---

### ComparaÃ§Ã£o das MÃ©tricas

| Aspecto                    | BLEU          | chrF          | COMET          | BERTScore      |
|----------------------------|:-------------:|:-------------:|:--------------:|:--------------:|
| **Granularidade**          | Palavra       | Caractere     | SentenÃ§a       | Subpalavra     |
| **Base de comparaÃ§Ã£o**     | N-gramas exatos | Char n-gramas | Embeddings XLM-R | Embeddings BERT |
| **Detecta sinÃ´nimos?**     | NÃ£o           | Parcialmente  | Sim            | Sim            |
| **Detecta parÃ¡frases?**    | NÃ£o           | NÃ£o           | Sim            | Sim            |
| **SensÃ­vel Ã  morfologia?** | NÃ£o           | Sim           | Sim            | Sim            |
| **Usa frase fonte?**       | NÃ£o           | NÃ£o           | Sim            | NÃ£o            |
| **CorrelaÃ§Ã£o com humanos** | Moderada      | Boa           | Muito alta     | Alta           |
| **Custo computacional**    | Muito baixo   | Muito baixo   | Alto (~2GB GPU)| MÃ©dio (~1GB GPU)|
| **Velocidade**             | ~5s/corpus    | ~5s/corpus    | ~60s/corpus    | ~30s/corpus    |
| **Interpretabilidade**     | Alta          | Alta          | Baixa (caixa-preta) | MÃ©dia     |
| **Ano de publicaÃ§Ã£o**      | 2002          | 2015          | 2022           | 2020           |

### Por que usar 4 mÃ©tricas?

```
Caso 1: BLEU alto, COMET baixo
â†’ A traduÃ§Ã£o tem as mesmas palavras, mas em ordem ou contexto errado
   Exemplo: "bank" traduzido como "banco" (financeiro) quando o contexto era "rio"

Caso 2: BLEU baixo, BERTScore alto
â†’ A traduÃ§Ã£o usa sinÃ´nimos/parÃ¡frases corretos que BLEU nÃ£o reconhece
   Exemplo: "febre" vs "temperatura elevada"

Caso 3: chrF alto, BLEU baixo
â†’ Morfologia correta mas palavras diferentes (flexÃµes, conjugaÃ§Ãµes)
   Exemplo: "apresentaram" vs "apresentou" (chrF captura "apresent-")

Caso 4: Todas altas
â†’ TraduÃ§Ã£o de alta qualidade âœ… (nosso caso: BLEU=45.51, chrF=70.54,
   COMET=0.8756, BERTScore=0.9124 apÃ³s fine-tuning)
```

### Resultados neste projeto

| MÃ©trica    | Antes  | Depois | Delta   | O que a melhoria indica                                    |
|------------|-------:|-------:|--------:|-----------------------------------------------------------|
| BLEU       | 40.06  | 45.51  | +5.45   | Mais n-gramas corretos â†’ vocabulÃ¡rio do domÃ­nio aprendido  |
| chrF       | 65.61  | 70.54  | +4.93   | Melhor morfologia â†’ concordÃ¢ncia e acentuaÃ§Ã£o aprendidas   |
| COMET      | 0.8499 | 0.8756 | +0.0257 | Maior adequaÃ§Ã£o semÃ¢ntica validada por modelo neural        |
| BERTScore  | 0.8957 | 0.9124 | +0.0167 | Embeddings mais prÃ³ximos â†’ significado melhor preservado    |

> **InterpretaÃ§Ã£o geral**: As 4 mÃ©tricas melhoraram de forma consistente, indicando que o fine-tuning produziu ganhos reais em *todas* as dimensÃµes de qualidade â€” nÃ£o apenas em sobreposiÃ§Ã£o lexical superficial, mas tambÃ©m em adequaÃ§Ã£o semÃ¢ntica profunda.

---

## ReferÃªncias

### Artigos CientÃ­ficos

#### MÃ©tricas de AvaliaÃ§Ã£o
- Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). *BLEU: a Method for Automatic Evaluation of Machine Translation*. In Proceedings of the 40th Annual Meeting of the ACL, pp. 311â€“318. https://aclanthology.org/P02-1040/
- PopoviÄ‡, M. (2015). *chrF: character n-gram F-score for automatic MT evaluation*. In Proceedings of the Tenth Workshop on Statistical Machine Translation (WMT), pp. 392â€“395. https://aclanthology.org/W15-3049/
- Post, M. (2018). *A Call for Clarity in Reporting BLEU Scores*. In Proceedings of the Third Conference on Machine Translation (WMT), pp. 186â€“191. https://aclanthology.org/W18-6319/
- Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., & Artzi, Y. (2020). *BERTScore: Evaluating Text Generation with BERT*. In International Conference on Learning Representations (ICLR 2020). https://openreview.net/forum?id=SkeHuCVFDr
- Conneau, A. et al. (2020). *Unsupervised Cross-lingual Representation Learning at Scale*. In Proceedings of ACL 2020, pp. 8440â€“8451. https://aclanthology.org/2020.acl-main.747/
- Rei, R. et al. (2022). *COMET-22: Unbabel-IST 2022 Submission for the Metrics Shared Task*. In Proceedings of the Seventh Conference on Machine Translation (WMT), pp. 578â€“585. https://aclanthology.org/2022.wmt-1.52/

#### Arquitetura e Modelos
- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Å., & Polosukhin, I. (2017). *Attention is All You Need*. In Advances in Neural Information Processing Systems (NeurIPS 2017), pp. 5998â€“6008. https://arxiv.org/abs/1706.03762
- Raffel, C. et al. (2019). *Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer*. arXiv:1910.10683. https://arxiv.org/abs/1910.10683
- Shaw, P., Uszkoreit, J., & Vaswani, A. (2018). *Self-Attention with Relative Position Representations*. In Proceedings of NAACL-HLT 2018, pp. 464â€“468. https://aclanthology.org/N18-2074/
- Lopes, A. et al. (2020). *Lite Training Strategies for Portuguese-English and English-Portuguese Translation*. In Proceedings of WMT 2020, pp. 833â€“840. https://aclanthology.org/2020.wmt-1.90/

#### TokenizaÃ§Ã£o e PrÃ©-processamento
- Kudo, T. & Richardson, J. (2018). *SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing*. In Proceedings of EMNLP 2018, pp. 66â€“71. https://aclanthology.org/D18-2012/

#### OtimizaÃ§Ã£o e Treinamento
- Kingma, D. P. & Ba, J. (2014). *Adam: A Method for Stochastic Optimization*. In International Conference on Learning Representations (ICLR 2015). https://arxiv.org/abs/1412.6980
- Loshchilov, I. & Hutter, F. (2019). *Decoupled Weight Decay Regularization*. In International Conference on Learning Representations (ICLR 2019). https://arxiv.org/abs/1711.05101
- Smith, L. N. (2018). *A disciplined approach to neural network hyper-parameters: Part 1 â€“ learning rate, batch size, momentum, and weight decay*. arXiv:1803.09820. https://arxiv.org/abs/1803.09820
- Goyal, P. et al. (2017). *Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour*. arXiv:1706.02677. https://arxiv.org/abs/1706.02677
- Howard, J. & Ruder, S. (2018). *Universal Language Model Fine-tuning for Text Classification*. In Proceedings of ACL 2018, pp. 328â€“339. https://aclanthology.org/P18-1031/

#### RegularizaÃ§Ã£o
- Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). *Dropout: A Simple Way to Prevent Neural Networks from Overfitting*. Journal of Machine Learning Research, 15(1), pp. 1929â€“1958.
- Prechelt, L. (1998). *Early Stopping â€” But When?*. In Neural Networks: Tricks of the Trade, Lecture Notes in Computer Science, vol 1524, pp. 55â€“69. https://doi.org/10.1007/3-540-49430-8_3

#### Batch Size e Escala
- Masters, D. & Luschi, C. (2018). *Revisiting Small Batch Training for Deep Neural Networks*. arXiv:1804.07612. https://arxiv.org/abs/1804.07612
- Keskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy, M., & Tang, P. T. P. (2017). *On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima*. In International Conference on Learning Representations (ICLR 2017). https://arxiv.org/abs/1609.04836
- Ott, M. et al. (2018). *Scaling Neural Machine Translation*. In Proceedings of the Third Conference on Machine Translation (WMT), pp. 1â€“9. https://aclanthology.org/W18-6301/

#### PrecisÃ£o Mista e EficiÃªncia
- Micikevicius, P. et al. (2018). *Mixed Precision Training*. In International Conference on Learning Representations (ICLR 2018). https://arxiv.org/abs/1710.03740
- Chen, T., Xu, B., Zhang, C., & Guestrin, C. (2016). *Training Deep Nets with Sublinear Memory Cost*. arXiv:1604.06174. https://arxiv.org/abs/1604.06174

#### Beam Search e DecodificaÃ§Ã£o
- Freitag, M. & Al-Onaizan, Y. (2017). *Beam Search Strategies for Neural Machine Translation*. In Proceedings of the First Workshop on Neural Machine Translation, pp. 56â€“60. https://aclanthology.org/W17-3207/

#### Fine-tuning e AdaptaÃ§Ã£o de DomÃ­nio
- Miceli Barone, A. V., Haddow, B., Germann, U., & Sennrich, R. (2017). *Regularization techniques for fine-tuning in neural machine translation*. In Proceedings of EMNLP 2017, pp. 1489â€“1494. https://aclanthology.org/D17-1156/
- Freitag, M. & Al-Onaizan, Y. (2016). *Fast Domain Adaptation for Neural Machine Translation*. arXiv:1612.06897
- Neubig, G. & Hu, J. (2018). *Rapid Adaptation of Neural Machine Translation to New Languages*. In Proceedings of EMNLP 2018, pp. 875â€“880. https://aclanthology.org/D18-1103/
- Koehn, P. & Knowles, R. (2017). *Six Challenges for Neural Machine Translation*. In Proceedings of the First Workshop on Neural Machine Translation, pp. 28â€“39. https://aclanthology.org/W17-3204/

#### LLMs e TraduÃ§Ã£o
- Zhu, W. et al. (2023). *Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis*. In Findings of NAACL 2024. arXiv:2304.04675
- Xu, H. et al. (2023). *A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models*. In ICLR 2024. arXiv:2309.11674

#### Livros-texto
- Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press. https://www.deeplearningbook.org/

### Bibliotecas e Ferramentas

- HuggingFace Transformers: https://huggingface.co/docs/transformers/
- SacreBLEU: https://github.com/mjpost/sacrebleu
- COMET: https://github.com/Unbabel/COMET
- BERTScore: https://github.com/Tiiiger/bert_score
- RepositÃ³rio do modelo: https://huggingface.co/unicamp-dl/translation-en-pt-t5
- CÃ³digo-fonte do modelo: https://github.com/unicamp-dl/Lite-T5-Translation

---

**VersÃ£o**: 7.0 | **Data**: Fevereiro 2026
